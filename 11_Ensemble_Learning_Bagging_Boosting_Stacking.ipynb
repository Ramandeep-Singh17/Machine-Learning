{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjjPxMBOQ9ey4Yki1M3C/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/Ensemble_Learning_Bagging_Boosting_Stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G58BmRtVgSe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🤖 Ensemble Learning in ML\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What is Ensemble Learning?\n",
        "\n",
        "**Ensemble Learning** ek technique hai jisme hum **multiple models** (learners) ko combine karte hain  \n",
        "taaki final prediction **individual model se better** ho.\n",
        "\n",
        "🧠 Tu ne sahi likha:\n",
        "> \"Hum bahut saare models train karte hain, aur unka output milakar voting ya average lete hain.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🕰️ When to Use?\n",
        "\n",
        "- Jab ek model se ache result nahi mil rahe ho  \n",
        "- Jab model **underfit ya overfit** kar raha ho  \n",
        "- Jab hume **high accuracy aur stability** chahiye ho\n",
        "\n",
        "---\n",
        "\n",
        "### ❓ Why use Ensemble Learning?\n",
        "\n",
        "- ✅ Accuracy improve hoti hai  \n",
        "- ✅ Model zyada generalize karta hai (overfit nahi karta easily)  \n",
        "- ✅ Har model ki galti doosra model cover kar leta hai  \n",
        "- ✅ Noise aur bias reduce hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Where is it used?\n",
        "\n",
        "- 🎯 Kaggle Competitions (Top models ensemble hi hote hain)\n",
        "- 🏥 Disease Prediction (multiple model ka opinion lena = safe diagnosis)\n",
        "- 💸 Credit Scoring / Fraud Detection (stable prediction chahiye)\n",
        "- 📈 Stock Price Prediction (alag models alag signals pakadte hain)\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How does it work?\n",
        "\n",
        "1. Multiple base models train karo (same ya different algorithms)\n",
        "2. Sab model apna prediction denge\n",
        "3. Final result:\n",
        "   - 🗳️ Classification → Voting (Majority wins)\n",
        "   - 📊 Regression → Averaging (Mean result)\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Self Note :\n",
        "\n",
        "> ✅ \"Hum bahut sare models banate hain aur data ko train karte hain,  \n",
        "> uske baad hum voting ke base pe final prediction nikalte hain.\"\n",
        "\n",
        "Bilkul sahi logic — ye hi hota hai ensemble ka **core idea** 🔥\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Diagram (Simple Voting Example):\n",
        "\n",
        "Model 1 → Class A,\n",
        "Model 2 → Class B,\n",
        "Model 3 → Class A\n",
        "\n",
        "🔚 Final Prediction = Class A (Majority Vote)   \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Real-Life Examples of Ensemble Learning\n",
        "\n",
        "---\n",
        "\n",
        "📈 **Stock Market Prediction**  \n",
        "- Alag-alag models market ke different signals ko detect karte hain  \n",
        "- Final decision ensemble se liya jata hai — zyada stable prediction milta hai\n",
        "\n",
        "🏥 **Medical Diagnosis (Disease Detection)**  \n",
        "- Logistic Regression + Random Forest + SVM ka ensemble use hota hai  \n",
        "- Agar sab ka majority prediction \"Disease Positive\" aaye, to wo final hota hai  \n",
        "- Real hospital systems me aise models lagaye jaate hain\n",
        "\n",
        "🏦 **Credit Scoring / Loan Approval**  \n",
        "- Different models: Decision Tree, Gradient Boosting, Naive Bayes  \n",
        "- Inka combine prediction use karke bataya jata hai ki customer reliable hai ya nahi\n",
        "\n",
        "🎯 **Kaggle Competitions**  \n",
        "- Top rankers mostly ensemble use karte hain (stacking ya blending)  \n",
        "- Accuracy badhane ke liye multiple tuned models ko ek saath combine karte hain\n",
        "\n",
        "📱 **Spam Detection (Email/Message)**  \n",
        "- Alag-alag classifiers (Naive Bayes, Random Forest, etc.) ka ensemble  \n",
        "- Final prediction: Spam ya Ham — more robust result\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 Summary:\n",
        "\n",
        "| Concept        | Description                                      |\n",
        "|----------------|--------------------------------------------------|\n",
        "| Goal           | Accuracy + Stability improve karna               |\n",
        "| Strategy       | Multiple model ka prediction combine karna       |\n",
        "| Output         | Voting (classification) / Averaging (regression) |\n",
        "| Advantage      | Better than single model                         |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPL7mS5u_gx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Aso used in Regrssion\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tZ7Zbk9wBTbk",
        "outputId": "68e87dce-e615-431b-f239-b9c71e40859a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aso used in Regrssion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 📊 Regression me Ensemble Learning kaise kaam karta hai?\n",
        "\n",
        "🧠 Ensemble Learning sirf classification me hi nahi, **regression problems** me bhi use hota hai.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Difference in Final Output Logic:\n",
        "\n",
        "| Problem Type     | Final Prediction Method     |\n",
        "|------------------|-----------------------------|\n",
        "| Classification   | 🗳️ **Voting** (Majority wins) |\n",
        "| Regression       | ➕ **Averaging** (Mean of all outputs)\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Example:\n",
        "\n",
        "Suppose 3 models ne predict kiya:\n",
        "\n",
        "- Model 1 → 95  \n",
        "- Model 2 → 90  \n",
        "- Model 3 → 100  \n",
        "\n",
        "🔚 Final Output = (95 + 90 + 100) / 3 = **95**\n",
        "\n",
        "---\n",
        "\n",
        "### 📍 Models used in Regression Ensemble:\n",
        "- Random Forest Regressor  \n",
        "- Gradient Boosting Regressor  \n",
        "- XGBoost Regressor  \n",
        "- Bagging Regressor  \n",
        "- Stacking Regressor\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Real-life Use Case:\n",
        "\n",
        "🏡 **House Price Prediction**  \n",
        "- Alag-alag regressors ka ensemble use hota hai  \n",
        "- Final price prediction zyada accurate hota hai (average nikal kar)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "w7ZgtpFXBzqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#types (Bagging, Boosting, Stacking)"
      ],
      "metadata": {
        "id": "-Aq8X4KIByb7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 📦 Ensemble Learning ke 3 Main Types\n",
        "\n",
        "1. **Bagging** (Bootstrap Aggregating)\n",
        "   - Multiple models → Parallel training → Final Voting/Averaging\n",
        "\n",
        "2. **Boosting**\n",
        "   - Weak learners → Sequential training → Each model fixes previous errors\n",
        "\n",
        "3. **Stacking**\n",
        "   - Different types of models → Combine predictions → One meta-model makes final decision\n",
        "\n",
        "---\n",
        "\n",
        "Next: Let's dive into **Bagging** ⬇️\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Ntcuiz0LB_MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Bagging\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YbD9PbIECiui",
        "outputId": "64432e65-db02-4839-a28e-23b5f006c948"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bagging'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🛍️ Bagging (Bootstrap Aggregation)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What is Bagging?\n",
        "\n",
        "Bagging ek ensemble technique hai jisme:\n",
        "\n",
        "- Hum **same type ke multiple models** banate hain (e.g., Decision Tree, SVM)\n",
        "- Har model ko **data ke alag random subset** par train karte hain (bootstrapped sampling)\n",
        "- Final output nikalta hai:\n",
        "  - **Voting** se (classification ke liye)\n",
        "  - **Averaging** se (regression ke liye)\n",
        "\n",
        "📌 \"Bootstrap\" = Random sampling with replacement  \n",
        "📌 \"Aggregation\" = Predictions ka combination\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why use Bagging?\n",
        "\n",
        "- ✅ Overfitting ko control karta hai (variance reduce hota hai)  \n",
        "- ✅ Model ki stability & consistency improve karta hai  \n",
        "- ✅ Noisy ya outlier-prone data pe bhi achha perform karta hai  \n",
        "- ✅ Parallel training se training time reduce hota hai  \n",
        "- ✅ Underfitting ke chances kam hote hain\n",
        "\n",
        "---\n",
        "\n",
        "### 🕰️ When to use Bagging?\n",
        "\n",
        "- Jab model high variance show karta ho (e.g., Decision Trees)  \n",
        "- Jab model training data pe overfit ho raha ho  \n",
        "- Jab data noisy ho ya bahut saare irrelevant features ho  \n",
        "- Jab stable aur generalized output chahiye\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Where is Bagging used?\n",
        "\n",
        "- 🎯 Random Forest classifiers & regressors  \n",
        "- 🏥 Medical diagnosis systems  \n",
        "- 🛒 Product recommendation systems  \n",
        "- 🏦 Credit risk prediction  \n",
        "- 📈 Stock market risk modeling  \n",
        "- 🚨 Fraud detection systems\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How does Bagging Work?\n",
        "\n",
        "1. Original data se **multiple bootstrapped subsets** banaye jaate hain  \n",
        "2. Har subset par **same algorithm** ka ek model train kiya jaata hai  \n",
        "3. Har model se prediction liya jaata hai  \n",
        "4. Final prediction:\n",
        "   - Classification → **Majority Vote**\n",
        "   - Regression → **Average of predictions**\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Example: (Tere handwritten explanation se inspired)\n",
        "\n",
        "Maan lo humare paas 2000 data points hain.\n",
        "\n",
        "- Humne 4 models banaye → sab SVM algorithm se  \n",
        "- Har model ko randomly selected 1000 data points pe train kiya  \n",
        "- Phir unse alag-alag queries pe prediction liya:\n",
        "\n",
        "| Query | Model 1 | Model 2 | Model 3 | Model 4 | Final Output |\n",
        "|--------|--------|--------|--------|--------|---------------|\n",
        "|   Q1   |   1    |   1    |   0    |   1    | ✅ 1 (Majority) |\n",
        "|   Q2   |   0    |   1    |   0    |   0    | ❌ 0 (Majority) |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 🖼️ Diagram: Bagging Flow (Same Algorithm – SVM Example)\n",
        "\n",
        "             Original Dataset (2000 rows)\n",
        "                        ↓\n",
        "        ┌───────────────┼───────────────┐\n",
        "        ↓               ↓               ↓\n",
        "     Subset 1        Subset 2        Subset 3        ... Subset 4  \n",
        "   (1000 rows)     (1000 rows)     (1000 rows)         (random)\n",
        "        ↓               ↓               ↓                  ↓\n",
        "     Model 1          Model 2         Model 3           Model 4  \n",
        "      (SVM)            (SVM)           (SVM)             (SVM)\n",
        "        ↓               ↓               ↓                  ↓\n",
        "\n",
        "        🔽 Final Prediction = Voting (Classification) / Averaging (Regression)\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Real-Life Examples — Bagging & Random Forest\n",
        "\n",
        "🏥 **Disease Diagnosis**  \n",
        "- Multiple trees lagake symptoms ke basis pe decision  \n",
        "- Majority vote se final prediction\n",
        "\n",
        "🏦 **Loan Default Prediction**  \n",
        "- Customer ke data pe Random Forest apply hota hai  \n",
        "- Risk accurately predict hota hai\n",
        "\n",
        "🛒 **Product Recommendation**  \n",
        "- Past user data se forest banake recommend karta hai\n",
        "\n",
        "📈 **Stock Risk Forecasting**  \n",
        "- Historical trends pe train karke bagging model risk predict karta hai\n",
        "\n",
        "🚨 **Fraud Detection**  \n",
        "- Random Forest transaction features ko analyze karta hai  \n",
        "- Suspicious activity detect hoti hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🌲 Why Random Forest is So Powerful?\n",
        "\n",
        "✅ **1. Handles Overfitting Well**  \n",
        "- Ek single Decision Tree overfit kar sakta hai  \n",
        "- RF me multiple trees ka average liya jaata hai → zyada stable result\n",
        "\n",
        "✅ **2. Classification & Regression Dono Me Kaam Karta Hai**  \n",
        "- Binary/multiclass classification + regression values\n",
        "\n",
        "✅ **3. Missing Values Ko Handle Kar Leta Hai**  \n",
        "- Incomplete data hone ke bawajood bhi performance achhi rehti hai\n",
        "\n",
        "✅ **4. Feature Importance Batata Hai**  \n",
        "- RF automatically identify karta hai kaunsa feature model ke liye important hai\n",
        "\n",
        "✅ **5. Scalable & Parallel**  \n",
        "- Bada data, zyada features — easily handle karta hai  \n",
        "- Parallel tree training = speed boost\n",
        "\n",
        "✅ **6. Hyperparameter Tuning Friendly**  \n",
        "- `n_estimators`, `max_depth`, etc. tune karke aur improve kar sakte hain\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary Table\n",
        "\n",
        "| Feature            | Bagging                              |\n",
        "|---------------------|----------------------------------------|\n",
        "| Model Type         | Same model multiple times (e.g. DTs)  |\n",
        "| Sampling           | Bootstrapped subsets                  |\n",
        "| Output             | Voting (classification), Avg (regression) |\n",
        "| Overfitting        | Greatly reduced                       |\n",
        "| Best Use-Case      | Random Forest                         |\n",
        "| Real Use Cases     | Diagnosis, fraud detection, recommendation |\n",
        "| Speed              | Fast (parallel)                       |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lP3SNjuLC3Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"BOOSTING\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u27aWyvnCk_B",
        "outputId": "bb824135-7007-4a0b-a280-6ddce6d154ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BOOSTING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🚀 Boosting – Ensemble Learning ka Power Mode (Final Colab Notes)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What is Boosting?\n",
        "\n",
        "**Boosting** ek powerful **ensemble learning technique** hai jisme:\n",
        "\n",
        "* Hum **multiple weak learners** (usually shallow Decision Trees) ko **sequentially** train karte hain\n",
        "* Har new model **pehle wale model ki galtiyon (errors)** ko sudharne ki koshish karta hai\n",
        "* Final prediction **sab models ke weighted combination** se hota hai\n",
        "\n",
        "📌 Boosting ka goal: Weak learners → Strong learner banana\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why Boosting?\n",
        "\n",
        "* ✅ Model ki accuracy gradually improve hoti hai\n",
        "* ✅ Har step me model errors ko fix karta hai\n",
        "* ✅ Complex aur non-linear patterns bhi easily learn karta hai\n",
        "* ✅ Bias aur variance dono ko reduce karta hai\n",
        "* ✅ Real-world me high performance ke liye best choice hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🕰️ When to Use Boosting?\n",
        "\n",
        "* Jab simple models underfit kar rahe ho\n",
        "* Jab accuracy training se test tak significantly improve karni ho\n",
        "* Jab data complex ho ya patterns clear na ho\n",
        "* Jab production-grade ya competition-level performance chahiye\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Where is Boosting Used?\n",
        "\n",
        "* 🎯 Kaggle Competitions → (Top 10 models boosting-based)\n",
        "* 🏥 Medical Diagnosis → (Cancer, Disease Prediction)\n",
        "* 🏦 Loan Risk Prediction → (Credit Default)\n",
        "* 📈 Stock Prediction, CTR in Ads\n",
        "* 🛒 Product Recommendations\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How Boosting Works (Based on Handwritten Notes)\n",
        "\n",
        "📘 Maan lo:\n",
        "\n",
        "* 2000 data points hai\n",
        "* 3 Models: M1, M2, M3\n",
        "\n",
        "🔁 Boosting ka Process:\n",
        "\n",
        "1. **Model 1 (M1)** → Full data pe train hota hai → kuch galat predictions honge\n",
        "2. **Model 2 (M2)** → M1 ke galat predict kiye gaye data points pe zyada focus karta hai\n",
        "3. **Model 3 (M3)** → M2 ke remaining errors se aur better seekhta hai\n",
        "4. ✅ **Final prediction** → sab models ke **weighted output** ka combination\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Diagram: Boosting Flow (M1 → M2 → M3)\n",
        "\n",
        "```\n",
        "Original Dataset (2000 rows)\n",
        "        ↓\n",
        "  Train Model 1 (M1)\n",
        "        ↓\n",
        "     Errors\n",
        "        ↓\n",
        "  Train Model 2 (M2) on M1 Errors\n",
        "        ↓\n",
        "     Errors\n",
        "        ↓\n",
        "  Train Model 3 (M3) on M2 Errors\n",
        "        ↓\n",
        "Final Prediction = Weighted Output (M1, M2, M3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🌟 Real-Life Examples\n",
        "\n",
        "🏥 **Cancer Prediction**:\n",
        "\n",
        "* Boosting models gene-level data ko deeply analyze karte hain\n",
        "* False negatives kam hote hain → reliable diagnosis\n",
        "\n",
        "🏦 **Credit Fraud Detection**:\n",
        "\n",
        "* Complex fraud patterns (jo rare hote hain) ko detect karta hai\n",
        "\n",
        "🛍️ **Product Recommendation**:\n",
        "\n",
        "* User ke click + purchase data se XGBoost personalized suggestions deta hai\n",
        "\n",
        "🎯 **Ad CTR Prediction**:\n",
        "\n",
        "* Complex user behavior learn kar ke accurate click prediction karta hai\n",
        "\n",
        "---\n",
        "\n",
        "## 🔥 Popular Boosting Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "### 1️⃣ ✅ AdaBoost (Adaptive Boosting)\n",
        "\n",
        "* Pehla popular boosting method\n",
        "* Galat classified samples ko zyada weight deta hai\n",
        "* Simple yet effective technique\n",
        "\n",
        "📌 Use: Face Detection, Spam Filtering\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ ⚡ Gradient Boosting\n",
        "\n",
        "* Har model residual errors (gradients) ko minimize karta hai\n",
        "* Gradient Descent logic se model errors fix karta hai\n",
        "\n",
        "📌 Use: Risk modeling, regression analysis, healthcare\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ 🚀 XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "> 🔥 Most powerful and widely used boosting technique\n",
        "\n",
        "* Gradient Boosting ka optimized version (C++ backend)\n",
        "* Super fast + Regularization support\n",
        "* Missing values ko handle karta hai\n",
        "* Early stopping, feature importance, and cross-validation built-in\n",
        "\n",
        "📌 Use:\n",
        "\n",
        "* Kaggle Competitions\n",
        "* Credit Scoring\n",
        "* Disease Risk Modeling\n",
        "* Ranking & CTR Systems\n",
        "\n",
        "---\n",
        "\n",
        "### 🌟 Why XGBoost is So Popular?\n",
        "\n",
        "| Reason                   | Description                                      |\n",
        "| ------------------------ | ------------------------------------------------ |\n",
        "| 🚀 Speed                 | C++ backend + parallel processing = Fast         |\n",
        "| 🧠 Regularization        | Overfitting avoid karta hai                      |\n",
        "| ✅ Missing Value Handling | Built-in smart handling                          |\n",
        "| 📊 Feature Importance    | Top features ko highlight karta hai              |\n",
        "| 🔄 Early Stopping        | Performance monitor kar ke timely stop karta hai |\n",
        "| 🏆 Competition King      | Kaggle, Zindi, Hackathons me top performer       |\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Summary Table\n",
        "\n",
        "| Algorithm         | Highlights                      | Use Cases                   |\n",
        "| ----------------- | ------------------------------- | --------------------------- |\n",
        "| AdaBoost          | Simple, weight-based correction | Face, Spam Detection        |\n",
        "| Gradient Boosting | Gradient error correction       | Medical, Credit, Regression |\n",
        "| XGBoost           | Fast + Regularized + Accurate   | All high-stakes ML problems |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JdzTeCEhEmbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"STACKING\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "veG8PeMoEiyN",
        "outputId": "ba1b2091-8490-45ac-b308-4f348d3a9906"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'STACKING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 🧱 Stacking (Stacked Generalization)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What is Stacking?\n",
        "\n",
        "**Stacking** ek ensemble technique hai jisme:\n",
        "- Multiple **different algorithms** (like Logistic Regression, SVM, Decision Tree) ek sath use kiye jaate hain\n",
        "- Har model apna prediction deta hai\n",
        "- In sab models ke prediction ko **ek naya model (meta-model)** final decision ke liye use karta hai\n",
        "\n",
        "📌 Tere words me:\n",
        "> \"Hum models ki performance ko stack karte hain, aur final decision ek aur model leta hai\"\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why use Stacking?\n",
        "\n",
        "- ✅ Combines strengths of different models\n",
        "- ✅ Har model alag tarike se pattern samajhta hai → overall smarter prediction\n",
        "- ✅ Underfitting & overfitting dono se bachne me help karta hai\n",
        "- ✅ Final model bias-variance trade-off balance karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🕰️ When to Use?\n",
        "\n",
        "- Jab individual models ache perform kar rahe ho, but consistent nahi ho  \n",
        "- Jab models alag nature ke ho (like tree-based + linear + margin-based)\n",
        "- Jab hume highest possible performance chahiye (competitions / production)\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Where is it used?\n",
        "\n",
        "- 🧬 Disease risk classification (stacking medical models)  \n",
        "- 📊 Credit default prediction  \n",
        "- 🎯 Kaggle competitions (top submissions often use stacking)  \n",
        "- 📈 Stock market trend forecasting  \n",
        "- 🛒 Click-through prediction in ad-tech\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ How Does Stacking Work?\n",
        "\n",
        "1. Pehle level pe:\n",
        "   - **Multiple base models** (e.g., Logistic Regression, SVM, Decision Tree) train kiye jaate hain\n",
        "   - Ye models **predictions generate** karte hain on same input data\n",
        "\n",
        "2. Doosre level pe:\n",
        "   - In base models ke predictions ko le kar ek **meta-model** (e.g., KNN) train kiya jaata hai\n",
        "   - Final output meta-model ke through nikalta hai\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Example (Tere Notes Se Inspired):\n",
        "\n",
        "| Input Data (Features) | LR | SVM | DT | → Meta-model (KNN) |\n",
        "|------------------------|----|-----|----|---------------------|\n",
        "| Query 1                | 1  | 1   | 0  | → 1                 |\n",
        "| Query 2                | 0  | 1   | 1  | → 1                 |\n",
        "| Query 3                | 0  | 0   | 1  | → 0                 |\n",
        "\n",
        "- Yaha base models (LR, SVM, DT) ka prediction ek feature ban gaya  \n",
        "- KNN un outputs ke pattern se final prediction karta hai\n",
        "\n",
        "---\n",
        "\n",
        "📌 Future Prediction:\n",
        "\n",
        "- Har base model ko ek **score / weight** assign hota hai  \n",
        "- Jiska score high hota hai, uska prediction meta-model zyada consider karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🤖 Classification + Regression Dono Me Use Hota Hai\n",
        "\n",
        "- 📚 Classification → Output = 0 ya 1  \n",
        "- 📈 Regression → Output = average numeric value  \n",
        "- Meta-model automatically decide karta hai kaise combine karna hai\n",
        "\n",
        "---\n",
        "\n",
        "### 📍 Real-Life Examples of Stacking:\n",
        "\n",
        "---\n",
        "\n",
        "🏥 **Medical Diagnosis**  \n",
        "- LR, SVM, DT lagate hain symptoms ke upar  \n",
        "- Meta-model unke decision ko combine karta hai → final diagnosis\n",
        "\n",
        "🏦 **Credit Risk Scoring**  \n",
        "- Logistic Regression + Random Forest + XGBoost ka ensemble  \n",
        "- Meta-model (e.g. Gradient Boosting) final credit score batata hai\n",
        "\n",
        "🎯 **Kaggle Competitions**  \n",
        "- Best performing models stacking hi karte hain  \n",
        "- Top-10 ke kaafi submissions stacking + meta-model hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Summary Table\n",
        "\n",
        "| Layer        | Models Used                  |\n",
        "|--------------|-------------------------------|\n",
        "| Base Layer   | LR, SVM, DT, etc.             |\n",
        "| Meta Layer   | KNN / Gradient Boosting / RF  |\n",
        "| Output       | Final Prediction              |\n",
        "\n",
        "---\n",
        "\n",
        "📌 Note:\n",
        "> Stacking is like using the wisdom of all models — sabka prediction le kar ek **smart final decision** liya jata hai.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Pz1oL0HeHFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comparison between all three"
      ],
      "metadata": {
        "id": "h9vsbdT0HElF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 📊 Comparison: Bagging vs Boosting vs Stacking\n",
        "\n",
        "| Feature                 | 🛍️ Bagging                          | 🚀 Boosting                          | 🧱 Stacking                            |\n",
        "|-------------------------|-------------------------------------|--------------------------------------|----------------------------------------|\n",
        "| 🔁 Training Type        | Parallel (same time)                | Sequential (one after another)       | Two-level (base + meta model)          |\n",
        "| 🧠 Goal                 | Reduce Variance                     | Reduce Bias + Variance               | Combine strengths of different models  |\n",
        "| 🧪 Error Handling       | Random sampling → avg prediction    | Each model corrects previous errors  | Meta-model learns from base outputs    |\n",
        "| 🔍 Data Subsets         | Bootstrapped samples                | Full data, focus on errors           | Full data for base models              |\n",
        "| 🏗️ Models Used         | Usually same type (e.g., DTs)       | Weak learners (e.g., shallow DTs)    | Different models (LR, SVM, RF, etc.)   |\n",
        "| 🎯 Final Output         | Majority vote / Average             | Weighted sum of learners             | Prediction by meta-model               |\n",
        "| 📉 Overfitting Control  | Yes (low variance)                  | Yes (with regularization)            | Depends on meta-model                  |\n",
        "| 🧠 Interpretability     | Medium                              | Low                                  | Medium to Low                          |\n",
        "| ⚙️ Performance          | High (stable)                       | Very High (can overfit)              | Very High (best of all worlds)         |\n",
        "| ⚡ Speed                | Fast (parallel)                     | Slower (sequential)                  | Medium (multi-stage)                   |\n",
        "| 🧪 Classification/Regression | ✅ Both                         | ✅ Both                               | ✅ Both                                 |\n",
        "| 🏆 Real-life Example    | Random Forest (Medical, Ecom)       | XGBoost (Credit, Kaggle)             | Kaggle top models, Complex apps        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-OvorL0tHPVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 🧠 Want to Explore More?\n",
        "\n",
        "- 🔗 [Scikit-learn Ensemble Learning Documentation](https://scikit-learn.org/stable/modules/ensemble.html)\n",
        "- 🔗 [XGBoost Official Documentation](https://xgboost.readthedocs.io/en/stable/)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sqp0vXWjOcXe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuwwLEobOdyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
