{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjjPxMBOQ9ey4Yki1M3C/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/Ensemble_Learning_Bagging_Boosting_Stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G58BmRtVgSe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ¤– Ensemble Learning in ML\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” What is Ensemble Learning?\n",
        "\n",
        "**Ensemble Learning** ek technique hai jisme hum **multiple models** (learners) ko combine karte hain  \n",
        "taaki final prediction **individual model se better** ho.\n",
        "\n",
        "ğŸ§  Tu ne sahi likha:\n",
        "> \"Hum bahut saare models train karte hain, aur unka output milakar voting ya average lete hain.\"\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ•°ï¸ When to Use?\n",
        "\n",
        "- Jab ek model se ache result nahi mil rahe ho  \n",
        "- Jab model **underfit ya overfit** kar raha ho  \n",
        "- Jab hume **high accuracy aur stability** chahiye ho\n",
        "\n",
        "---\n",
        "\n",
        "### â“ Why use Ensemble Learning?\n",
        "\n",
        "- âœ… Accuracy improve hoti hai  \n",
        "- âœ… Model zyada generalize karta hai (overfit nahi karta easily)  \n",
        "- âœ… Har model ki galti doosra model cover kar leta hai  \n",
        "- âœ… Noise aur bias reduce hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Where is it used?\n",
        "\n",
        "- ğŸ¯ Kaggle Competitions (Top models ensemble hi hote hain)\n",
        "- ğŸ¥ Disease Prediction (multiple model ka opinion lena = safe diagnosis)\n",
        "- ğŸ’¸ Credit Scoring / Fraud Detection (stable prediction chahiye)\n",
        "- ğŸ“ˆ Stock Price Prediction (alag models alag signals pakadte hain)\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ How does it work?\n",
        "\n",
        "1. Multiple base models train karo (same ya different algorithms)\n",
        "2. Sab model apna prediction denge\n",
        "3. Final result:\n",
        "   - ğŸ—³ï¸ Classification â†’ Voting (Majority wins)\n",
        "   - ğŸ“Š Regression â†’ Averaging (Mean result)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Self Note :\n",
        "\n",
        "> âœ… \"Hum bahut sare models banate hain aur data ko train karte hain,  \n",
        "> uske baad hum voting ke base pe final prediction nikalte hain.\"\n",
        "\n",
        "Bilkul sahi logic â€” ye hi hota hai ensemble ka **core idea** ğŸ”¥\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Diagram (Simple Voting Example):\n",
        "\n",
        "Model 1 â†’ Class A,\n",
        "Model 2 â†’ Class B,\n",
        "Model 3 â†’ Class A\n",
        "\n",
        "ğŸ”š Final Prediction = Class A (Majority Vote)   \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Real-Life Examples of Ensemble Learning\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“ˆ **Stock Market Prediction**  \n",
        "- Alag-alag models market ke different signals ko detect karte hain  \n",
        "- Final decision ensemble se liya jata hai â€” zyada stable prediction milta hai\n",
        "\n",
        "ğŸ¥ **Medical Diagnosis (Disease Detection)**  \n",
        "- Logistic Regression + Random Forest + SVM ka ensemble use hota hai  \n",
        "- Agar sab ka majority prediction \"Disease Positive\" aaye, to wo final hota hai  \n",
        "- Real hospital systems me aise models lagaye jaate hain\n",
        "\n",
        "ğŸ¦ **Credit Scoring / Loan Approval**  \n",
        "- Different models: Decision Tree, Gradient Boosting, Naive Bayes  \n",
        "- Inka combine prediction use karke bataya jata hai ki customer reliable hai ya nahi\n",
        "\n",
        "ğŸ¯ **Kaggle Competitions**  \n",
        "- Top rankers mostly ensemble use karte hain (stacking ya blending)  \n",
        "- Accuracy badhane ke liye multiple tuned models ko ek saath combine karte hain\n",
        "\n",
        "ğŸ“± **Spam Detection (Email/Message)**  \n",
        "- Alag-alag classifiers (Naive Bayes, Random Forest, etc.) ka ensemble  \n",
        "- Final prediction: Spam ya Ham â€” more robust result\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“¦ Summary:\n",
        "\n",
        "| Concept        | Description                                      |\n",
        "|----------------|--------------------------------------------------|\n",
        "| Goal           | Accuracy + Stability improve karna               |\n",
        "| Strategy       | Multiple model ka prediction combine karna       |\n",
        "| Output         | Voting (classification) / Averaging (regression) |\n",
        "| Advantage      | Better than single model                         |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPL7mS5u_gx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Aso used in Regrssion\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tZ7Zbk9wBTbk",
        "outputId": "68e87dce-e615-431b-f239-b9c71e40859a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aso used in Regrssion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ğŸ“Š Regression me Ensemble Learning kaise kaam karta hai?\n",
        "\n",
        "ğŸ§  Ensemble Learning sirf classification me hi nahi, **regression problems** me bhi use hota hai.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” Difference in Final Output Logic:\n",
        "\n",
        "| Problem Type     | Final Prediction Method     |\n",
        "|------------------|-----------------------------|\n",
        "| Classification   | ğŸ—³ï¸ **Voting** (Majority wins) |\n",
        "| Regression       | â• **Averaging** (Mean of all outputs)\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Example:\n",
        "\n",
        "Suppose 3 models ne predict kiya:\n",
        "\n",
        "- Model 1 â†’ 95  \n",
        "- Model 2 â†’ 90  \n",
        "- Model 3 â†’ 100  \n",
        "\n",
        "ğŸ”š Final Output = (95 + 90 + 100) / 3 = **95**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ Models used in Regression Ensemble:\n",
        "- Random Forest Regressor  \n",
        "- Gradient Boosting Regressor  \n",
        "- XGBoost Regressor  \n",
        "- Bagging Regressor  \n",
        "- Stacking Regressor\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Real-life Use Case:\n",
        "\n",
        "ğŸ¡ **House Price Prediction**  \n",
        "- Alag-alag regressors ka ensemble use hota hai  \n",
        "- Final price prediction zyada accurate hota hai (average nikal kar)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "w7ZgtpFXBzqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#types (Bagging, Boosting, Stacking)"
      ],
      "metadata": {
        "id": "-Aq8X4KIByb7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ğŸ“¦ Ensemble Learning ke 3 Main Types\n",
        "\n",
        "1. **Bagging** (Bootstrap Aggregating)\n",
        "   - Multiple models â†’ Parallel training â†’ Final Voting/Averaging\n",
        "\n",
        "2. **Boosting**\n",
        "   - Weak learners â†’ Sequential training â†’ Each model fixes previous errors\n",
        "\n",
        "3. **Stacking**\n",
        "   - Different types of models â†’ Combine predictions â†’ One meta-model makes final decision\n",
        "\n",
        "---\n",
        "\n",
        "Next: Let's dive into **Bagging** â¬‡ï¸\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Ntcuiz0LB_MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Bagging\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YbD9PbIECiui",
        "outputId": "64432e65-db02-4839-a28e-23b5f006c948"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bagging'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ›ï¸ Bagging (Bootstrap Aggregation)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” What is Bagging?\n",
        "\n",
        "Bagging ek ensemble technique hai jisme:\n",
        "\n",
        "- Hum **same type ke multiple models** banate hain (e.g., Decision Tree, SVM)\n",
        "- Har model ko **data ke alag random subset** par train karte hain (bootstrapped sampling)\n",
        "- Final output nikalta hai:\n",
        "  - **Voting** se (classification ke liye)\n",
        "  - **Averaging** se (regression ke liye)\n",
        "\n",
        "ğŸ“Œ \"Bootstrap\" = Random sampling with replacement  \n",
        "ğŸ“Œ \"Aggregation\" = Predictions ka combination\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Why use Bagging?\n",
        "\n",
        "- âœ… Overfitting ko control karta hai (variance reduce hota hai)  \n",
        "- âœ… Model ki stability & consistency improve karta hai  \n",
        "- âœ… Noisy ya outlier-prone data pe bhi achha perform karta hai  \n",
        "- âœ… Parallel training se training time reduce hota hai  \n",
        "- âœ… Underfitting ke chances kam hote hain\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ•°ï¸ When to use Bagging?\n",
        "\n",
        "- Jab model high variance show karta ho (e.g., Decision Trees)  \n",
        "- Jab model training data pe overfit ho raha ho  \n",
        "- Jab data noisy ho ya bahut saare irrelevant features ho  \n",
        "- Jab stable aur generalized output chahiye\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Where is Bagging used?\n",
        "\n",
        "- ğŸ¯ Random Forest classifiers & regressors  \n",
        "- ğŸ¥ Medical diagnosis systems  \n",
        "- ğŸ›’ Product recommendation systems  \n",
        "- ğŸ¦ Credit risk prediction  \n",
        "- ğŸ“ˆ Stock market risk modeling  \n",
        "- ğŸš¨ Fraud detection systems\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ How does Bagging Work?\n",
        "\n",
        "1. Original data se **multiple bootstrapped subsets** banaye jaate hain  \n",
        "2. Har subset par **same algorithm** ka ek model train kiya jaata hai  \n",
        "3. Har model se prediction liya jaata hai  \n",
        "4. Final prediction:\n",
        "   - Classification â†’ **Majority Vote**\n",
        "   - Regression â†’ **Average of predictions**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Example: (Tere handwritten explanation se inspired)\n",
        "\n",
        "Maan lo humare paas 2000 data points hain.\n",
        "\n",
        "- Humne 4 models banaye â†’ sab SVM algorithm se  \n",
        "- Har model ko randomly selected 1000 data points pe train kiya  \n",
        "- Phir unse alag-alag queries pe prediction liya:\n",
        "\n",
        "| Query | Model 1 | Model 2 | Model 3 | Model 4 | Final Output |\n",
        "|--------|--------|--------|--------|--------|---------------|\n",
        "|   Q1   |   1    |   1    |   0    |   1    | âœ… 1 (Majority) |\n",
        "|   Q2   |   0    |   1    |   0    |   0    | âŒ 0 (Majority) |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ğŸ–¼ï¸ Diagram: Bagging Flow (Same Algorithm â€“ SVM Example)\n",
        "\n",
        "             Original Dataset (2000 rows)\n",
        "                        â†“\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â†“               â†“               â†“\n",
        "     Subset 1        Subset 2        Subset 3        ... Subset 4  \n",
        "   (1000 rows)     (1000 rows)     (1000 rows)         (random)\n",
        "        â†“               â†“               â†“                  â†“\n",
        "     Model 1          Model 2         Model 3           Model 4  \n",
        "      (SVM)            (SVM)           (SVM)             (SVM)\n",
        "        â†“               â†“               â†“                  â†“\n",
        "\n",
        "        ğŸ”½ Final Prediction = Voting (Classification) / Averaging (Regression)\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Real-Life Examples â€” Bagging & Random Forest\n",
        "\n",
        "ğŸ¥ **Disease Diagnosis**  \n",
        "- Multiple trees lagake symptoms ke basis pe decision  \n",
        "- Majority vote se final prediction\n",
        "\n",
        "ğŸ¦ **Loan Default Prediction**  \n",
        "- Customer ke data pe Random Forest apply hota hai  \n",
        "- Risk accurately predict hota hai\n",
        "\n",
        "ğŸ›’ **Product Recommendation**  \n",
        "- Past user data se forest banake recommend karta hai\n",
        "\n",
        "ğŸ“ˆ **Stock Risk Forecasting**  \n",
        "- Historical trends pe train karke bagging model risk predict karta hai\n",
        "\n",
        "ğŸš¨ **Fraud Detection**  \n",
        "- Random Forest transaction features ko analyze karta hai  \n",
        "- Suspicious activity detect hoti hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ² Why Random Forest is So Powerful?\n",
        "\n",
        "âœ… **1. Handles Overfitting Well**  \n",
        "- Ek single Decision Tree overfit kar sakta hai  \n",
        "- RF me multiple trees ka average liya jaata hai â†’ zyada stable result\n",
        "\n",
        "âœ… **2. Classification & Regression Dono Me Kaam Karta Hai**  \n",
        "- Binary/multiclass classification + regression values\n",
        "\n",
        "âœ… **3. Missing Values Ko Handle Kar Leta Hai**  \n",
        "- Incomplete data hone ke bawajood bhi performance achhi rehti hai\n",
        "\n",
        "âœ… **4. Feature Importance Batata Hai**  \n",
        "- RF automatically identify karta hai kaunsa feature model ke liye important hai\n",
        "\n",
        "âœ… **5. Scalable & Parallel**  \n",
        "- Bada data, zyada features â€” easily handle karta hai  \n",
        "- Parallel tree training = speed boost\n",
        "\n",
        "âœ… **6. Hyperparameter Tuning Friendly**  \n",
        "- `n_estimators`, `max_depth`, etc. tune karke aur improve kar sakte hain\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Summary Table\n",
        "\n",
        "| Feature            | Bagging                              |\n",
        "|---------------------|----------------------------------------|\n",
        "| Model Type         | Same model multiple times (e.g. DTs)  |\n",
        "| Sampling           | Bootstrapped subsets                  |\n",
        "| Output             | Voting (classification), Avg (regression) |\n",
        "| Overfitting        | Greatly reduced                       |\n",
        "| Best Use-Case      | Random Forest                         |\n",
        "| Real Use Cases     | Diagnosis, fraud detection, recommendation |\n",
        "| Speed              | Fast (parallel)                       |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lP3SNjuLC3Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"BOOSTING\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u27aWyvnCk_B",
        "outputId": "bb824135-7007-4a0b-a280-6ddce6d154ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BOOSTING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸš€ Boosting â€“ Ensemble Learning ka Power Mode (Final Colab Notes)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” What is Boosting?\n",
        "\n",
        "**Boosting** ek powerful **ensemble learning technique** hai jisme:\n",
        "\n",
        "* Hum **multiple weak learners** (usually shallow Decision Trees) ko **sequentially** train karte hain\n",
        "* Har new model **pehle wale model ki galtiyon (errors)** ko sudharne ki koshish karta hai\n",
        "* Final prediction **sab models ke weighted combination** se hota hai\n",
        "\n",
        "ğŸ“Œ Boosting ka goal: Weak learners â†’ Strong learner banana\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Why Boosting?\n",
        "\n",
        "* âœ… Model ki accuracy gradually improve hoti hai\n",
        "* âœ… Har step me model errors ko fix karta hai\n",
        "* âœ… Complex aur non-linear patterns bhi easily learn karta hai\n",
        "* âœ… Bias aur variance dono ko reduce karta hai\n",
        "* âœ… Real-world me high performance ke liye best choice hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ•°ï¸ When to Use Boosting?\n",
        "\n",
        "* Jab simple models underfit kar rahe ho\n",
        "* Jab accuracy training se test tak significantly improve karni ho\n",
        "* Jab data complex ho ya patterns clear na ho\n",
        "* Jab production-grade ya competition-level performance chahiye\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Where is Boosting Used?\n",
        "\n",
        "* ğŸ¯ Kaggle Competitions â†’ (Top 10 models boosting-based)\n",
        "* ğŸ¥ Medical Diagnosis â†’ (Cancer, Disease Prediction)\n",
        "* ğŸ¦ Loan Risk Prediction â†’ (Credit Default)\n",
        "* ğŸ“ˆ Stock Prediction, CTR in Ads\n",
        "* ğŸ›’ Product Recommendations\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ How Boosting Works (Based on Handwritten Notes)\n",
        "\n",
        "ğŸ“˜ Maan lo:\n",
        "\n",
        "* 2000 data points hai\n",
        "* 3 Models: M1, M2, M3\n",
        "\n",
        "ğŸ” Boosting ka Process:\n",
        "\n",
        "1. **Model 1 (M1)** â†’ Full data pe train hota hai â†’ kuch galat predictions honge\n",
        "2. **Model 2 (M2)** â†’ M1 ke galat predict kiye gaye data points pe zyada focus karta hai\n",
        "3. **Model 3 (M3)** â†’ M2 ke remaining errors se aur better seekhta hai\n",
        "4. âœ… **Final prediction** â†’ sab models ke **weighted output** ka combination\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§© Diagram: Boosting Flow (M1 â†’ M2 â†’ M3)\n",
        "\n",
        "```\n",
        "Original Dataset (2000 rows)\n",
        "        â†“\n",
        "  Train Model 1 (M1)\n",
        "        â†“\n",
        "     Errors\n",
        "        â†“\n",
        "  Train Model 2 (M2) on M1 Errors\n",
        "        â†“\n",
        "     Errors\n",
        "        â†“\n",
        "  Train Model 3 (M3) on M2 Errors\n",
        "        â†“\n",
        "Final Prediction = Weighted Output (M1, M2, M3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒŸ Real-Life Examples\n",
        "\n",
        "ğŸ¥ **Cancer Prediction**:\n",
        "\n",
        "* Boosting models gene-level data ko deeply analyze karte hain\n",
        "* False negatives kam hote hain â†’ reliable diagnosis\n",
        "\n",
        "ğŸ¦ **Credit Fraud Detection**:\n",
        "\n",
        "* Complex fraud patterns (jo rare hote hain) ko detect karta hai\n",
        "\n",
        "ğŸ›ï¸ **Product Recommendation**:\n",
        "\n",
        "* User ke click + purchase data se XGBoost personalized suggestions deta hai\n",
        "\n",
        "ğŸ¯ **Ad CTR Prediction**:\n",
        "\n",
        "* Complex user behavior learn kar ke accurate click prediction karta hai\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”¥ Popular Boosting Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "### 1ï¸âƒ£ âœ… AdaBoost (Adaptive Boosting)\n",
        "\n",
        "* Pehla popular boosting method\n",
        "* Galat classified samples ko zyada weight deta hai\n",
        "* Simple yet effective technique\n",
        "\n",
        "ğŸ“Œ Use: Face Detection, Spam Filtering\n",
        "\n",
        "---\n",
        "\n",
        "### 2ï¸âƒ£ âš¡ Gradient Boosting\n",
        "\n",
        "* Har model residual errors (gradients) ko minimize karta hai\n",
        "* Gradient Descent logic se model errors fix karta hai\n",
        "\n",
        "ğŸ“Œ Use: Risk modeling, regression analysis, healthcare\n",
        "\n",
        "---\n",
        "\n",
        "### 3ï¸âƒ£ ğŸš€ XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "> ğŸ”¥ Most powerful and widely used boosting technique\n",
        "\n",
        "* Gradient Boosting ka optimized version (C++ backend)\n",
        "* Super fast + Regularization support\n",
        "* Missing values ko handle karta hai\n",
        "* Early stopping, feature importance, and cross-validation built-in\n",
        "\n",
        "ğŸ“Œ Use:\n",
        "\n",
        "* Kaggle Competitions\n",
        "* Credit Scoring\n",
        "* Disease Risk Modeling\n",
        "* Ranking & CTR Systems\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒŸ Why XGBoost is So Popular?\n",
        "\n",
        "| Reason                   | Description                                      |\n",
        "| ------------------------ | ------------------------------------------------ |\n",
        "| ğŸš€ Speed                 | C++ backend + parallel processing = Fast         |\n",
        "| ğŸ§  Regularization        | Overfitting avoid karta hai                      |\n",
        "| âœ… Missing Value Handling | Built-in smart handling                          |\n",
        "| ğŸ“Š Feature Importance    | Top features ko highlight karta hai              |\n",
        "| ğŸ”„ Early Stopping        | Performance monitor kar ke timely stop karta hai |\n",
        "| ğŸ† Competition King      | Kaggle, Zindi, Hackathons me top performer       |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Summary Table\n",
        "\n",
        "| Algorithm         | Highlights                      | Use Cases                   |\n",
        "| ----------------- | ------------------------------- | --------------------------- |\n",
        "| AdaBoost          | Simple, weight-based correction | Face, Spam Detection        |\n",
        "| Gradient Boosting | Gradient error correction       | Medical, Credit, Regression |\n",
        "| XGBoost           | Fast + Regularized + Accurate   | All high-stakes ML problems |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JdzTeCEhEmbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"STACKING\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "veG8PeMoEiyN",
        "outputId": "ba1b2091-8490-45ac-b308-4f348d3a9906"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'STACKING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ§± Stacking (Stacked Generalization)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” What is Stacking?\n",
        "\n",
        "**Stacking** ek ensemble technique hai jisme:\n",
        "- Multiple **different algorithms** (like Logistic Regression, SVM, Decision Tree) ek sath use kiye jaate hain\n",
        "- Har model apna prediction deta hai\n",
        "- In sab models ke prediction ko **ek naya model (meta-model)** final decision ke liye use karta hai\n",
        "\n",
        "ğŸ“Œ Tere words me:\n",
        "> \"Hum models ki performance ko stack karte hain, aur final decision ek aur model leta hai\"\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Why use Stacking?\n",
        "\n",
        "- âœ… Combines strengths of different models\n",
        "- âœ… Har model alag tarike se pattern samajhta hai â†’ overall smarter prediction\n",
        "- âœ… Underfitting & overfitting dono se bachne me help karta hai\n",
        "- âœ… Final model bias-variance trade-off balance karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ•°ï¸ When to Use?\n",
        "\n",
        "- Jab individual models ache perform kar rahe ho, but consistent nahi ho  \n",
        "- Jab models alag nature ke ho (like tree-based + linear + margin-based)\n",
        "- Jab hume highest possible performance chahiye (competitions / production)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ Where is it used?\n",
        "\n",
        "- ğŸ§¬ Disease risk classification (stacking medical models)  \n",
        "- ğŸ“Š Credit default prediction  \n",
        "- ğŸ¯ Kaggle competitions (top submissions often use stacking)  \n",
        "- ğŸ“ˆ Stock market trend forecasting  \n",
        "- ğŸ›’ Click-through prediction in ad-tech\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ How Does Stacking Work?\n",
        "\n",
        "1. Pehle level pe:\n",
        "   - **Multiple base models** (e.g., Logistic Regression, SVM, Decision Tree) train kiye jaate hain\n",
        "   - Ye models **predictions generate** karte hain on same input data\n",
        "\n",
        "2. Doosre level pe:\n",
        "   - In base models ke predictions ko le kar ek **meta-model** (e.g., KNN) train kiya jaata hai\n",
        "   - Final output meta-model ke through nikalta hai\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… Example (Tere Notes Se Inspired):\n",
        "\n",
        "| Input Data (Features) | LR | SVM | DT | â†’ Meta-model (KNN) |\n",
        "|------------------------|----|-----|----|---------------------|\n",
        "| Query 1                | 1  | 1   | 0  | â†’ 1                 |\n",
        "| Query 2                | 0  | 1   | 1  | â†’ 1                 |\n",
        "| Query 3                | 0  | 0   | 1  | â†’ 0                 |\n",
        "\n",
        "- Yaha base models (LR, SVM, DT) ka prediction ek feature ban gaya  \n",
        "- KNN un outputs ke pattern se final prediction karta hai\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“Œ Future Prediction:\n",
        "\n",
        "- Har base model ko ek **score / weight** assign hota hai  \n",
        "- Jiska score high hota hai, uska prediction meta-model zyada consider karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¤– Classification + Regression Dono Me Use Hota Hai\n",
        "\n",
        "- ğŸ“š Classification â†’ Output = 0 ya 1  \n",
        "- ğŸ“ˆ Regression â†’ Output = average numeric value  \n",
        "- Meta-model automatically decide karta hai kaise combine karna hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“ Real-Life Examples of Stacking:\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ¥ **Medical Diagnosis**  \n",
        "- LR, SVM, DT lagate hain symptoms ke upar  \n",
        "- Meta-model unke decision ko combine karta hai â†’ final diagnosis\n",
        "\n",
        "ğŸ¦ **Credit Risk Scoring**  \n",
        "- Logistic Regression + Random Forest + XGBoost ka ensemble  \n",
        "- Meta-model (e.g. Gradient Boosting) final credit score batata hai\n",
        "\n",
        "ğŸ¯ **Kaggle Competitions**  \n",
        "- Best performing models stacking hi karte hain  \n",
        "- Top-10 ke kaafi submissions stacking + meta-model hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§± Summary Table\n",
        "\n",
        "| Layer        | Models Used                  |\n",
        "|--------------|-------------------------------|\n",
        "| Base Layer   | LR, SVM, DT, etc.             |\n",
        "| Meta Layer   | KNN / Gradient Boosting / RF  |\n",
        "| Output       | Final Prediction              |\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“Œ Note:\n",
        "> Stacking is like using the wisdom of all models â€” sabka prediction le kar ek **smart final decision** liya jata hai.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Pz1oL0HeHFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comparison between all three"
      ],
      "metadata": {
        "id": "h9vsbdT0HElF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ğŸ“Š Comparison: Bagging vs Boosting vs Stacking\n",
        "\n",
        "| Feature                 | ğŸ›ï¸ Bagging                          | ğŸš€ Boosting                          | ğŸ§± Stacking                            |\n",
        "|-------------------------|-------------------------------------|--------------------------------------|----------------------------------------|\n",
        "| ğŸ” Training Type        | Parallel (same time)                | Sequential (one after another)       | Two-level (base + meta model)          |\n",
        "| ğŸ§  Goal                 | Reduce Variance                     | Reduce Bias + Variance               | Combine strengths of different models  |\n",
        "| ğŸ§ª Error Handling       | Random sampling â†’ avg prediction    | Each model corrects previous errors  | Meta-model learns from base outputs    |\n",
        "| ğŸ” Data Subsets         | Bootstrapped samples                | Full data, focus on errors           | Full data for base models              |\n",
        "| ğŸ—ï¸ Models Used         | Usually same type (e.g., DTs)       | Weak learners (e.g., shallow DTs)    | Different models (LR, SVM, RF, etc.)   |\n",
        "| ğŸ¯ Final Output         | Majority vote / Average             | Weighted sum of learners             | Prediction by meta-model               |\n",
        "| ğŸ“‰ Overfitting Control  | Yes (low variance)                  | Yes (with regularization)            | Depends on meta-model                  |\n",
        "| ğŸ§  Interpretability     | Medium                              | Low                                  | Medium to Low                          |\n",
        "| âš™ï¸ Performance          | High (stable)                       | Very High (can overfit)              | Very High (best of all worlds)         |\n",
        "| âš¡ Speed                | Fast (parallel)                     | Slower (sequential)                  | Medium (multi-stage)                   |\n",
        "| ğŸ§ª Classification/Regression | âœ… Both                         | âœ… Both                               | âœ… Both                                 |\n",
        "| ğŸ† Real-life Example    | Random Forest (Medical, Ecom)       | XGBoost (Credit, Kaggle)             | Kaggle top models, Complex apps        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-OvorL0tHPVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ğŸ§  Want to Explore More?\n",
        "\n",
        "- ğŸ”— [Scikit-learn Ensemble Learning Documentation](https://scikit-learn.org/stable/modules/ensemble.html)\n",
        "- ğŸ”— [XGBoost Official Documentation](https://xgboost.readthedocs.io/en/stable/)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "sqp0vXWjOcXe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuwwLEobOdyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
