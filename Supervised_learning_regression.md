# ðŸ“˜ Supervised Machine Learning â€” Regression (Markdown Notes for GitHub)

---

## ðŸ§  What is Supervised Machine Learning?

Supervised Machine Learning is an approach where the model is trained on **labeled data** â€” that is, both **inputs (X)** and their corresponding **outputs (Y)** are known.

### ðŸ“Œ How it Works:

* **Input:** Feature(s) (X)
* **Output:** Label (Y)
* The model learns the mapping `f(X) â†’ Y`
* Then it predicts Y for new/unseen X

### ðŸ§ª Types of Supervised Learning:

| Type           | Description                      | Example                              |
| -------------- | -------------------------------- | ------------------------------------ |
| Regression     | Output is **continuous**         | Salary prediction, house price       |
| Classification | Output is **categorical/labels** | Spam detection, tumor classification |

### âœ… Why is it Important?

* Output is **clearly known** â†’ more effective training
* Covers most real-world problems (finance, healthcare, etc.)
* Foundation for most ML algorithms

### ðŸŒ Applications:

| Domain     | Use Case                           |
| ---------- | ---------------------------------- |
| Finance    | Stock prediction, fraud detection  |
| Healthcare | Disease detection (classification) |
| E-commerce | Product recommendation             |
| Email      | Spam vs Non-Spam filtering         |
| Marketing  | Customer churn prediction          |

---

## ðŸ“ˆ Linear Regression

Linear Regression is a supervised ML algorithm used to predict a **continuous target variable** using one or more input variables.

### ðŸ“Œ Best-Fit Line:

> The goal is to find the **best-fit line**: `y = mx + c`

Where:

* `y` = predicted value
* `x` = input feature
* `m` = slope (how much y changes per unit x)
* `c` = intercept (value of y when x = 0)

#### ðŸ–Šï¸ Example:

> Predicting salary (`y`) based on years of experience (`x`).

![Linear Regression Diagram](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

---

## ðŸ“‰ Residuals & Cost Function

### ðŸ“Œ Residual:

> The **difference** between the actual value and the predicted value:

```
Residual = Actual y - Predicted y
```

### ðŸ“Œ Cost Function:

> Measures total error over all training points (commonly used: **MSE**)

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}_i - y_i)^2
$$

Goal: **Minimize cost function** by adjusting slope & intercept.

---

## ðŸ” Gradient Descent

> Optimization algorithm used to **minimize the cost function**

### âš™ï¸ How it works:

* Start with random `Î¸â‚€` and `Î¸â‚`
* Calculate gradient (slope of cost function)
* Update `Î¸` values to move toward **global minimum**

### ðŸ“‰ Visual:

```
         |
 Cost    |     .-'''
         |  .-'
         |/__________
              Theta
```

### âš ï¸ Learning Rate (Î±):

* Controls step size in each iteration
* Too small â†’ slow learning
* Too large â†’ may overshoot minimum

---

## ðŸ“Š Model Evaluation Metrics

| Metric      | Use                                  |
| ----------- | ------------------------------------ |
| RÂ² Score    | % of variance explained by the model |
| Adjusted RÂ² | Adjusted for number of features      |
| MAE         | Mean of absolute errors              |
| MSE         | Mean of squared errors               |
| RMSE        | Square root of MSE                   |

### ðŸ“ˆ RÂ² vs Adjusted RÂ²:

* **Adjusted RÂ²** is more reliable when there are multiple features.

---

## ðŸ¤– Multiple Linear Regression

> When we have **multiple input features**, it's called **Multiple Regression**.

Equation:

```
y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

Use cases: Salary prediction based on age, experience, education, etc.

---

## ðŸš§ Underfitting vs Overfitting

| Situation        | Train Accuracy | Test Accuracy | Bias | Variance |
| ---------------- | -------------- | ------------- | ---- | -------- |
| Underfitting     | âŒ Low          | âŒ Low         | High | Low      |
| Overfitting      | âœ… High         | âŒ Low         | Low  | High     |
| Good Fit (Ideal) | âœ… High         | âœ… High        | Low  | Low      |

---

## ðŸ§± Regularization

> Techniques to avoid overfitting by **penalizing large coefficients**

### ðŸ“Œ Types:

* **Ridge Regression (L2)**: Adds squared penalty to cost function
* **Lasso Regression (L1)**: Adds absolute value penalty

Equation:

```
Cost = MSE + Î» * (Î£|Î¸|)  â† Lasso
Cost = MSE + Î» * (Î£Î¸Â²)   â† Ridge
```

---

## âœ… Summary

> Linear Regression is the base of most ML regression tasks. It's easy to interpret, mathematically elegant, and scalable. Real power comes when you combine theory (as youâ€™ve written) with real-world code and projects.

---

## ðŸ”š Coming Next:

* Logistic Regression
* Polynomial Regression
* Feature Scaling
* Model Tuning
* Real Projects with sklearn & pandas

---

*Created by combining handwritten notes + Colab theory comments. Ready to be pushed on GitHub with pride ðŸš€*
