{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHT5141zuKk7pvLe2NDwQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/Supervised_learning_classification_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.what is classifiaction algorithm?**\n",
        "\n",
        "A supervised learning algorithm that predicts categories or class labels (like yes/no, spam/ham, cat/dog).\n",
        "\n",
        "Example:\n",
        "\n",
        "Email spam detection ‚Äî output: spam or not spam\n",
        "\n",
        "Real-life Use:\n",
        "\n",
        "Medical diagnosis (disease / no disease)\n",
        "\n",
        "Fraud detection (fraud / not fraud)\n",
        "\n",
        "Image recognition (dog / cat / car)\n",
        "\n",
        "Why use:\n",
        "When the output is categorical, not numerical."
      ],
      "metadata": {
        "id": "yIRpHuY-42qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression.\n",
        "same linear regression ki tarah hi line banegi but ab ye straight nhi rahega ye Squashed line banegi S-shaped curve (sigmoid function).\n",
        "\n",
        "That converts linear output into a probability between 0 and 1.\n",
        "\n",
        "**Why squashed?**\n",
        "\n",
        "Because instead of giving any value from ‚àí‚àû to +‚àû like linear regression, it compresses (squashes) everything into a bounded range [0, 1].\n",
        "\n",
        "To create the best fit line we use y=mx+c (yaha hm theta(x)=thetha0 +theta1X1)\n",
        "\n",
        "*why we dont use straight line?*\n",
        "\n",
        "1. It is because ki ho sakta hai ki bas ek ***outlier*** se pura priction kharab ho jaye straight line me.\n",
        "\n",
        "2. logistic regression predicts probabilities, which must be between 0 and 1.\n",
        "A straight line (like in linear regression) can go below 0 or above 1, which makes no sense for probability.\n",
        "\n",
        "To prevent these thing we squashed the line in S-shaped curve (sigmoid activation function).\n",
        "\n",
        "Formula: $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "\n",
        "where;\n",
        "\n",
        "e= eulers number. It‚Äôs a constant ‚âà 2.718.\n",
        "\n",
        "It is used for giving smooth curve.\n",
        "\n",
        "\"Z\"=It's the linear combination of inputs, like:\n",
        "\n",
        "$$ z = w_1 x_1 + w_2 x_2 + \\dots + b $$\n",
        "\n",
        "\n",
        "\n",
        "Use:\n",
        "To interpret output as probability for classification (e.g., 0.85 ‚Üí 85% chance of class 1).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IIwJozEh5jg1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RtEERTUB99S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Linear vs Logistic Regression ‚Äì 5 Most Important Differences\n",
        "\n",
        "1. **Best Fit Curve**:  \n",
        "   - Linear Regression uses Œ∏‚ÇÄ and Œ∏‚ÇÅ to draw a **straight line**.  \n",
        "   - Logistic Regression fits a **squashed S-shaped sigmoid curve**.\n",
        "\n",
        "2. **Error & Cost Function**:  \n",
        "   - Linear Regression calculates **residual error** and minimizes **Mean Squared Error (MSE)** using **Gradient Descent**.  \n",
        "   - Logistic Regression uses **Log Loss (Cross Entropy)** as its cost function with **Gradient Descent**.\n",
        "\n",
        "3. **Output Type**:  \n",
        "   - Linear ‚Üí Gives **continuous output** (e.g., 73.5)  \n",
        "   - Logistic ‚Üí Gives **probability** (between 0 and 1)\n",
        "\n",
        "4. **Use Case**:  \n",
        "   - Linear ‚Üí Used for **regression tasks** like price prediction, score estimation  \n",
        "   - Logistic ‚Üí Used for **classification tasks** like spam detection, disease prediction\n",
        "\n",
        "5. **Function Used**:  \n",
        "   - Linear ‚Üí Uses equation: **y = Œ∏‚ÇÄ + Œ∏‚ÇÅx**  \n",
        "   - Logistic ‚Üí Uses **sigmoid function**: œÉ(z) = 1 / (1 + e^(-z))\n"
      ],
      "metadata": {
        "id": "uTgT4KorEaxs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40U8R0WqEeZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Log loss function**\n",
        "\n",
        "Log Loss (or Binary Cross Entropy) measures how far the predicted probability is from the actual label (0 or 1).\n",
        "It penalizes wrong confident predictions more heavily.\n",
        "\n",
        "### üîπ Log Loss Function (Binary Cross Entropy)\n",
        "\n",
        "Let:\n",
        "- y = actual label (0 or 1)\n",
        "- p = predicted probability (between 0 and 1)\n",
        "\n",
        "Then,\n",
        "\n",
        "Loss = - [ y * log(p) + (1 - y) * log(1 - p) ]\n",
        "\n",
        "\n",
        "‚úÖ Example:\n",
        "If actual = 1, predicted = 0.9 ‚Üí loss = very small (good prediction).\n",
        "\n",
        "If actual = 1, predicted = 0.1 ‚Üí loss = large (bad prediction)."
      ],
      "metadata": {
        "id": "CX4K6G5kUSgj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YlS1MhtaUzKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Log Loss Intuition (Binary Cross Entropy)\n",
        "\n",
        "Let:\n",
        "- y = actual label (0 or 1)\n",
        "- p = predicted probability (between 0 and 1)\n",
        "\n",
        "Loss = - [ y * log(p) + (1 - y) * log(1 - p) ]\n",
        "\n",
        "---\n",
        "\n",
        "üî∏ Case 1: Correct & Confident Prediction\n",
        "\n",
        "Actual (y) = 1  \n",
        "Predicted Probability (p) = 0.99  \n",
        "\n",
        "Loss = - [1 * log(0.99) + 0 * log(1 - 0.99)]  \n",
        "     ‚âà -log(0.99)  \n",
        "     ‚âà **very small loss (~0.01)** ‚úÖ (Good prediction)\n",
        "\n",
        "üßæ Model ne bola 99% chance hai ki label 1 hai ‚Üí aur sach me 1 hi tha ‚Üí to loss bahut kam\n",
        "\n",
        "---\n",
        "\n",
        "üî∏ Case 2: Wrong & Confident Prediction\n",
        "\n",
        "Actual (y) = 1  \n",
        "Predicted Probability (p) = 0.01  \n",
        "\n",
        "Loss = - [1 * log(0.01) + 0 * log(1 - 0.01)]  \n",
        "     ‚âà -log(0.01)  \n",
        "     ‚âà **high loss (~4.6)** ‚ùå (Bad prediction)\n",
        "\n",
        "üßæ Model ne bola sirf 1% chance hai ki label 1 hai ‚Üí lekin actual me 1 hi tha ‚Üí to zyada penalty\n",
        "\n",
        "---\n",
        "\n",
        "üî∏ Conclusion:\n",
        "\n",
        "- ‚úÖ Correct + confident ‚Üí low loss  \n",
        "- ‚ùå Wrong + confident ‚Üí high loss\n"
      ],
      "metadata": {
        "id": "IVl4KsiRU0Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iVUA8tfU04z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Log Loss Function (Binary Cross Entropy)\n",
        "\n",
        "Log Loss is used in classification to measure how well the predicted probability matches the actual label.\n",
        "\n",
        "Formula:\n",
        "Loss =$$\n",
        "\\text{LogLoss} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- y = actual label (0 or 1)\n",
        "- ≈∑ = predicted probability (between 0 and 1)\n",
        "- m = number of samples (for total dataset loss, average over m)\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Case 1: Correct & Confident Prediction\n",
        "\n",
        "y = 1  \n",
        "≈∑ = 0.99\n",
        "\n",
        "Loss = -log(0.99) ‚âà **very low loss (~0.01)** ‚úÖ  \n",
        "üìå Good, confident, correct prediction ‚Üí small penalty\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Case 2: Wrong & Confident Prediction\n",
        "\n",
        "y = 1  \n",
        "≈∑ = 0.01\n",
        "\n",
        "Loss = -log(0.01) ‚âà **very high loss (~4.6)** ‚ùå  \n",
        "üìå Confident but wrong prediction ‚Üí large penalty\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Mid Prediction Example\n",
        "\n",
        "y = 1  \n",
        "≈∑ = 0.56\n",
        "\n",
        "Loss = -log(0.56) ‚âà **0.579**  \n",
        "üìå Medium confidence ‚Üí medium penalty\n",
        "\n",
        "---\n",
        "\n",
        "üßæ **Hinglish Summary**:\n",
        "- Agar model confidently galat ho ‚Üí zyada loss\n",
        "- Confidently sahi ho ‚Üí kam loss\n",
        "- Loss hamesha **log ke through** calculate hota hai\n",
        "- Goal = **Loss ko minimum karna** using gradient descent ‚Üí to reach **global minima**\n",
        "---\n",
        "Hm gradient function ke help se minimun point ko khujte hai and use global minima banate hai .\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MH_XKR5BVgUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "iqfmxYFiZUCq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4fBageqZsj1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}