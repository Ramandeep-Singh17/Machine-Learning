{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6edcASe533oyX+xplRZK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/All_classification_method_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä Confusion Matrix in Classification\n",
        "\n",
        "A confusion matrix is a 2x2 table used to **evaluate the performance** of a classification model.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What We Use (Structure):\n",
        "\n",
        "|               | Predicted Positive | Predicted Negative |\n",
        "|---------------|--------------------|--------------------|\n",
        "| Actual Positive | True Positive (TP)  | False Negative (FN) |\n",
        "| Actual Negative | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why We Use It\n",
        "\n",
        "- To **analyze model's prediction quality** beyond simple accuracy  \n",
        "- Helps us understand **types of errors** a model is making  \n",
        "- Basis for calculating **precision, recall, F1-score**\n",
        "\n",
        "üßæ **Hinglish**: Sirf accuracy se kaam nahi chalta ‚Äî confusion matrix batata hai model galti kis type ki kar raha hai.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How We Use It\n",
        "\n",
        "1. Model banate hain (e.g., logistic regression)\n",
        "2. Test data pe prediction chalate hain\n",
        "3. Compare karte hain predicted labels vs actual labels\n",
        "4. Fill the matrix: TP, FP, FN, TN\n",
        "5. Calculate metrics like:\n",
        "   - **Accuracy** = (TP + TN) / Total\n",
        "         accuracy is the percentage of correct prediction.\n",
        "   - **Precision** = TP / (TP + FP)\n",
        "         out of all predicted +ve,how many actually +ve.\n",
        "         use when false +ve are costly.(spam detection).\n",
        "   - **Recall** = TP / (TP + FN)\n",
        "         out of all actual +ve, how many we actually predicted.\n",
        "         use when false -ve are costly.(disease detection)\n",
        "   - **F1 Score** = Harmonic mean of precision and recall.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ‚è∞ When We Use It\n",
        "\n",
        "- For any **classification task**  \n",
        "- Specially when **class imbalance** ho (e.g., 90% no, 10% yes)\n",
        "- Jab **false positives** ya **false negatives** zyada important ho\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Real-life Examples\n",
        "\n",
        "1. **Spam Detection**  \n",
        "   - TP: Spam email correctly marked as spam  \n",
        "   - FP: Normal email marked as spam (bad!)\n",
        "\n",
        "2. **Medical Diagnosis**  \n",
        "   - FN: Sick person predicted as healthy (very risky!)  \n",
        "   - FP: Healthy person predicted as sick (extra tests)\n",
        "\n",
        "3. **Loan Approval**  \n",
        "   - TP: Right customer got loan  \n",
        "   - FN: Good customer rejected (loss of business)\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Final Thoughts\n",
        "\n",
        "‚úÖ Use confusion matrix to deeply understand how your model behaves  \n",
        "‚ö†Ô∏è Precision-Recall tradeoff important depending on use-case  \n",
        "üß† Accuracy alone is misleading sometimes!\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rnAxqknFgBWi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KviQW_PhhSWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéØ F1 Score ‚Äì Formula and Meaning\n",
        "\n",
        "F1 Score is the **harmonic mean** of Precision and Recall.  \n",
        "It balances both metrics ‚Äî useful when you need a trade-off.\n",
        "\n",
        "Use when you want balance between precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîπ F1 Score Formula:\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- **Precision** = TP / (TP + FP)  \n",
        "- **Recall** = TP / (TP + FN)\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚úÖ Use F1 Score When:\n",
        "- You want a **balance** between precision and recall  \n",
        "- Useful in **imbalanced datasets** (e.g., fraud detection, disease classification)\n",
        "\n",
        "üßæ **Hinglish Tip**:  \n",
        "Agar Precision aur Recall dono important hain (aur unequal bhi hain), to **F1 Score best metric** hai ‚Äî ye dono ka average leke fair score deta hai.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dNMXDbLOjfv_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4C4M2iOjk0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö†Ô∏è Type I & Type II Errors ‚Äì Explained with Confusion Matrix\n",
        "\n",
        "Confusion Matrix:\n",
        "\n",
        "|                     | Predicted Positive | Predicted Negative |\n",
        "|---------------------|--------------------|--------------------|\n",
        "| **Actual Positive** | ‚úÖ True Positive (TP) | ‚ùå False Negative (FN) ‚Üê Type II Error |\n",
        "| **Actual Negative** | ‚ùå False Positive (FP) ‚Üê Type I Error | ‚úÖ True Negative (TN) |\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Type I Error (False Positive)\n",
        "\n",
        "- **Definition**: Model predicted **positive**, but actually it was **negative**\n",
        "- **Example**: Email marked as spam, but it was not spam  \n",
        "- **Hinglish**: Galti se \"haan\" bol diya jab \"na\" tha  \n",
        "- **Problem**: Leads to unnecessary actions (e.g., wrong alerts)\n",
        "\n",
        "üîß **How to reduce**: Increase **precision** (be more sure before predicting positive)\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Type II Error (False Negative)\n",
        "\n",
        "- **Definition**: Model predicted **negative**, but actually it was **positive**\n",
        "- **Example**: Cancer patient predicted as healthy üò±  \n",
        "- **Hinglish**: Galti se \"na\" bol diya jab \"haan\" tha  \n",
        "- **Problem**: Missed detection ‚Üí can be dangerous\n",
        "\n",
        "üîß **How to reduce**: Increase **recall** (catch more positives even if a few wrong)\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Tradeoff:\n",
        "\n",
        "- Type I ‚¨ÜÔ∏è ‚Üí Type II ‚¨áÔ∏è and vice versa  \n",
        "- Balance depends on **problem type**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Real-life Analogy:\n",
        "\n",
        "| Scenario         | Type I Error                           | Type II Error                            |\n",
        "|------------------|----------------------------------------|------------------------------------------|\n",
        "| COVID Test       | Healthy person marked positive (false alarm) | Infected person marked negative (missed case) |\n",
        "| Spam Filter      | Real email goes to spam                | Spam email enters inbox                  |\n",
        "| Criminal Trial   | Innocent person punished               | Criminal goes free                       |\n",
        "\n"
      ],
      "metadata": {
        "id": "-sLdnQOclW48"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSrpWP7UlXlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå K-Nearest Neighbors (KNN)\n",
        "\n",
        "**Full Form**: KNN = K-Nearest Neighbors  \n",
        "KNN is a **supervised learning algorithm** used for both **classification** and **regression** tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is KNN?\n",
        "\n",
        "KNN predicts the output (class/value) of a data point by looking at the **'k' nearest data points** in the training set.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Why We Use KNN?\n",
        "\n",
        "- Simple and intuitive algorithm  \n",
        "- No training required (non-parametric)  \n",
        "- Works well with **small datasets**  \n",
        "- Useful when decision boundaries are irregular\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ When We Use KNN?\n",
        "\n",
        "- When you want quick predictions without a trained model  \n",
        "- When data is **not linearly separable**  \n",
        "- When you have features that are **distance-comparable**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ How KNN Works? (Steps)\n",
        "\n",
        "1. Choose value of **k** (e.g., k = 5)\n",
        "2. Calculate distance from the new point to all training data points\n",
        "3. Pick **k closest neighbors**\n",
        "4. Do **majority voting** (for classification) or **average** (for regression)\n",
        "5. Return the final result\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Real-Life Examples\n",
        "\n",
        "- Recommender systems (e.g., movies, shopping)  \n",
        "- Handwriting detection (e.g., digit recognition)  \n",
        "- Medical diagnosis (e.g., similar patient symptoms)  \n",
        "- Credit risk scoring\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Distance Calculation in KNN\n",
        "\n",
        "Two commonly used formulas:\n",
        "\n",
        "1. **Euclidean Distance** (üìå Most commonly used)\n",
        "\n",
        "$$\n",
        "d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
        "$$\n",
        "\n",
        "2. **Manhattan Distance**\n",
        "\n",
        "$$\n",
        "d = |x_1 - x_2| + |y_1 - y_2|\n",
        "$$\n",
        "\n",
        "üßæ Hinglish:  \n",
        "- Euclidean = Seedha line distance (diagonal)  \n",
        "- Manhattan = Block by block distance (jaise city streets)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Use in Classification & Regression\n",
        "\n",
        "- **Classification** ‚Üí Uses **majority voting** among neighbors  \n",
        "- **Regression** ‚Üí Takes **mean/average** of neighbors' values  \n",
        "‚úîÔ∏è That's why KNN supports **both tasks**\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ What does k=5 mean?\n",
        "\n",
        "It means model will look at **5 nearest neighbors** to make a prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ What is Hyperparameter?\n",
        "\n",
        "- A setting **you choose manually** before training  \n",
        "- In KNN, **k is a hyperparameter**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó Why k should be an odd number?\n",
        "\n",
        "To avoid **tie** in voting (especially in binary classification)  \n",
        "üßæ Ex: If k=4 and 2 are class A, 2 are class B ‚Üí tie ho jaayega\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Cross Validation (Short Intro)\n",
        "\n",
        "- It‚Äôs a method to **test different k values** on different data splits  \n",
        "- Helps to choose the **best k** by checking performance  \n",
        "üßæ Don‚Äôt go deep now ‚Äî just know it helps in choosing k fairly\n",
        "\n",
        "---\n",
        "\n",
        "### üåê What is n-Dimension?\n",
        "\n",
        "- Real-world data can have **multiple features** (age, salary, height, etc.)\n",
        "- So each data point lies in **n-dimensional space**  \n",
        "üßæ Ex: 3 features = 3D space, 10 features = 10D space\n",
        "\n",
        "KNN works in **any number of dimensions**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Extra Key Points:\n",
        "\n",
        "- KNN is a **lazy learner** ‚Üí no model built during training  \n",
        "- Sensitive to **feature scaling** ‚Üí use normalization (MinMax, StandardScaler)  \n",
        "- KNN slows down when dataset is **large** (computational cost ‚¨ÜÔ∏è)  \n",
        "- **Outliers** can affect prediction accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "uaJXKDq-mkqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#knn in hinglish"
      ],
      "metadata": {
        "id": "98ySyewIsAi4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå KNN ‚Äì K-Nearest Neighbors (Supervised Learning)\n",
        "\n",
        "**Full Form**: K-Nearest Neighbors  \n",
        "KNN ek simple aur powerful algorithm hai jo **classification** aur **regression** dono me use hota hai.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is KNN?\n",
        "\n",
        "KNN ek aisa algorithm hai jo naya data point ka output tab predict karta hai jab uske **k nearest neighbors** ke output dekhta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Why use KNN?\n",
        "\n",
        "- Bahut hi simple logic hai  \n",
        "- Training phase me kuch nahi hota (no model building = lazy learner)  \n",
        "- Chhoti datasets pe achha kaam karta hai  \n",
        "- Jab data ka shape irregular ho, tab bhi work karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ When use KNN?\n",
        "\n",
        "- Jab koi trained model ki zarurat na ho  \n",
        "- Jab data me clear separation na ho  \n",
        "- Jab distance based comparison possible ho\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ How KNN works? (Step-by-step)\n",
        "\n",
        "1. **k choose karo** (e.g., k = 5)\n",
        "2. New point ka distance calculate karo training data ke sabhi points se\n",
        "3. **k nearest** points select karo\n",
        "4. Classification ke liye **majority voting** karo  \n",
        "   Regression ke liye **average value** lo\n",
        "5. Predict karo final output\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Real-Life Examples\n",
        "\n",
        "- Movie suggestion system  \n",
        "- Digit recognition (jaise handwritten numbers)  \n",
        "- Medical diagnosis based on symptoms  \n",
        "- Loan default prediction\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Distance Kaise Nikalte Hain?\n",
        "\n",
        "2 famous formulas:\n",
        "\n",
        "1. **Euclidean Distance** (üìå Sabse zyada use hoti hai)\n",
        "\n",
        "$$\n",
        "d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
        "$$\n",
        "\n",
        "2. **Manhattan Distance**\n",
        "\n",
        "$$\n",
        "d = |x_1 - x_2| + |y_1 - y_2|\n",
        "$$\n",
        "\n",
        "üßæ **Samjho**:\n",
        "- Euclidean = seedha line distance (hypotenuse)  \n",
        "- Manhattan = blocks me ghumo jaise city ka road map\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Classification vs Regression\n",
        "\n",
        "- **Classification** ‚Üí Majority voting (jaise 3/5 neighbors \"Yes\" to predict Yes)  \n",
        "- **Regression** ‚Üí Average nikalte hain neighbors ki values ka\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì k = 5 ka kya matlab hai?\n",
        "\n",
        "Iska matlab tum prediction ke liye **5 sabse paas wale points** ke decision dekh rahe ho.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß What is Hyperparameter?\n",
        "\n",
        "- Aisi value jo tum khud set karte ho training se pehle  \n",
        "- **k value** is an example of hyperparameter in KNN\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è k odd number hona chahiye, kyun?\n",
        "\n",
        "Agar k even hoga to **tie** ho sakta hai majority voting me.  \n",
        "üßæ Example: k = 4 ‚Üí 2 yes, 2 no ‚Üí decision unclear\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Cross Validation (short idea)\n",
        "\n",
        "- Yeh ek method hai jisse tum **k ki best value** test kar sakte ho  \n",
        "- Alag-alag data pe k ko test karke pata chalta hai kaunsa best perform karta hai\n",
        "\n",
        "üßæ Abhi deep nahi ja rahe ‚Äî just remember: cross-validation = fair testing\n",
        "\n",
        "---\n",
        "\n",
        "### üåê n-Dimension kya hota hai?\n",
        "\n",
        "- Har feature (jaise age, salary, income...) ek dimension hoti hai  \n",
        "- 3 features ‚Üí 3D space  \n",
        "- 10 features ‚Üí 10D space\n",
        "\n",
        "KNN **multi-dimensional** data pe kaam karta hai using distance formula.\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Extra Important Points:\n",
        "\n",
        "- KNN is a **lazy learner** ‚Üí training time pe kuch nahi karta  \n",
        "- Feature scaling zaroori hai (use StandardScaler or MinMax)  \n",
        "- Large datasets me KNN **slow** hota hai (kyunki har baar sabka distance check karta hai)  \n",
        "- **Outliers** prediction accuracy ko impact kar sakte hain\n",
        "\n"
      ],
      "metadata": {
        "id": "C5Yu7MoT_lWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#naive bayes in hinglish"
      ],
      "metadata": {
        "id": "_iEopf6G_mAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå Naive Bayes ‚Äì Supervised Learning Algorithm\n",
        "\n",
        "Naive Bayes ek probabilistic classification algorithm hai jo **Bayes' Theorem** pe based hota hai, with a strong (naive) assumption:  \n",
        "üì¢ Features are **independent** of each other.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is Naive Bayes?\n",
        "\n",
        "- Ek **probability-based algorithm**  \n",
        "- Class prediction karta hai based on **likelihood of features**  \n",
        "- \"Naive\" kyun? ‚Üí Assume karta hai ki sab features ek dusre se independent hain\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Why Use Naive Bayes?\n",
        "\n",
        "- Bahut **fast & efficient** hota hai  \n",
        "- Best choice when features are text, binary, or categorical  \n",
        "- Works great for **text classification**, **spam filtering**, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ When to Use?\n",
        "\n",
        "- Jab features zyada hain but independent hain  \n",
        "- Jab fast and interpretable model chahiye  \n",
        "- Jab data me probabilistic pattern hai (jaise spam/no spam)\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ How it Works?\n",
        "\n",
        "1. Training data se **prior** probabilities nikaalte hain  \n",
        "2. Har feature ke liye **likelihood** calculate karte hain  \n",
        "3. **Bayes Theorem** use karke final probability nikalte hain  \n",
        "4. Jiska **highest probability**, us class ko predict karte hain\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Real-Life Examples\n",
        "\n",
        "- Email spam detection  \n",
        "- Sentiment analysis (positive/negative)  \n",
        "- Disease diagnosis  \n",
        "- News categorization  \n",
        "- Face recognition\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Bayes Theorem (Basic Idea)\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- **P(A|B)** = Posterior (Probability of A given B)  \n",
        "- **P(B|A)** = Likelihood  \n",
        "- **P(A)** = Prior  \n",
        "- **P(B)** = Evidence\n",
        "\n",
        "üßæ Hinglish:  \n",
        "Bayes theorem batata hai: agar mujhe B mila hai, to A hone ki kya chance hai?\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Probability Concepts (Simple)\n",
        "\n",
        "- **Independent Event**: Aisa event jiska result doosre se affect nahi hota  \n",
        "  Example: Coin toss and dice roll  \n",
        "- **Dependent Event**: Aisa event jiska result doosre pe depend karta hai  \n",
        "  Example: 2 cards pick without replacement\n",
        "\n",
        "üß† Naive Bayes assume karta hai ki **sab features independent hain** ‚Üí isiliye fast & simple calculation hoti hai\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Proof Reference from Your Notes\n",
        "\n",
        "> **P(A and B) = P(A) √ó P(B|A)**  \n",
        "> From this, we get Bayes Theorem by solving for **P(A|B)**\n",
        "\n",
        "Bas itna yaad rakho:  \n",
        "Bayes theorem ka proof conditional probability se derive hota hai using multiplication rule.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ How is Bayes Theorem Used in Classification?\n",
        "\n",
        "In Naive Bayes, we calculate:\n",
        "\n",
        "$$\n",
        "P(Class|Features) = \\frac{P(Features|Class) \\cdot P(Class)}{P(Features)}\n",
        "$$\n",
        "\n",
        "Phir har class ke liye yeh probability nikalte hain ‚Üí jiska **highest** hota hai, wahi predicted class ban jaata hai.\n",
        "\n",
        "üßæ Example:  \n",
        "Tumhare email me kuch specific words mile ‚Üí Naive Bayes calculate karega ki ye spam hone ki probability kitni hai.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Extra Helpful Points\n",
        "\n",
        "- Naive Bayes works well even with **small datasets**  \n",
        "- Best for **text data** (Bag of Words, TF-IDF)  \n",
        "- Doesn't need much training time (fast training)  \n",
        "- Sensitive to **zero probability** ‚Üí solve using **Laplace Smoothing**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary\n",
        "\n",
        "| Point            | Naive Bayes Meaning                                  |\n",
        "|------------------|------------------------------------------------------|\n",
        "| Based on         | Bayes Theorem                                        |\n",
        "| Assumption       | Features are independent                             |\n",
        "| Use Case         | Spam filter, sentiment, text classification          |\n",
        "| Type             | Supervised, classification                          |\n",
        "| Strength         | Fast, interpretable, great for high-dimension data   |\n"
      ],
      "metadata": {
        "id": "0eSgg5cE_2Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#naive bayes in english"
      ],
      "metadata": {
        "id": "XQsbpU-MKEZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå Naive Bayes ‚Äì Supervised Learning Algorithm\n",
        "\n",
        "Naive Bayes is a **probabilistic classifier** based on **Bayes' Theorem**, with a strong assumption that all features are **independent** of each other.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is Naive Bayes?\n",
        "\n",
        "- A **probability-based algorithm**  \n",
        "- It predicts the class of a data point using the **likelihood of features**  \n",
        "- Called \"Naive\" because it **assumes all features are independent**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Why Use Naive Bayes?\n",
        "\n",
        "- Extremely **fast and efficient**\n",
        "- Works well with **text and categorical data**\n",
        "- Performs great in **high-dimensional datasets**\n",
        "- Especially useful when the **data distribution is known or assumed**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ When to Use It?\n",
        "\n",
        "- When you need a **quick, interpretable model**  \n",
        "- For **text classification tasks**  \n",
        "- When your features are **conditionally independent**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ How Does Naive Bayes Work?\n",
        "\n",
        "1. Calculate **prior probabilities** from the training data  \n",
        "2. For each feature, calculate the **likelihood** given each class  \n",
        "3. Apply **Bayes' Theorem** to compute the **posterior probability**  \n",
        "4. Choose the class with the **highest probability**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Real-Life Examples\n",
        "\n",
        "- Email spam detection  \n",
        "- Sentiment classification (positive/negative)  \n",
        "- Disease diagnosis  \n",
        "- News topic categorization  \n",
        "- Face recognition\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Bayes' Theorem ‚Äì The Core Concept\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- **P(A|B)** ‚Üí Probability of A given B (posterior)  \n",
        "- **P(B|A)** ‚Üí Likelihood  \n",
        "- **P(A)** ‚Üí Prior probability  \n",
        "- **P(B)** ‚Üí Evidence (normalizing factor)\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ Probability Concepts You Should Know\n",
        "\n",
        "- **Independent Events**: Events where one doesn't affect the other  \n",
        "  _Example: Coin flip and rolling a dice_  \n",
        "- **Dependent Events**: One event's outcome affects the other  \n",
        "  _Example: Picking two cards without replacement_\n",
        "\n",
        "‚úÖ Naive Bayes assumes features are **independent**, which simplifies computation.\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Proof Reference (from your notes)\n",
        "\n",
        "From multiplication rule:\n",
        "\n",
        "$$\n",
        "P(A \\text{ and } B) = P(A) \\cdot P(B|A)\n",
        "$$\n",
        "\n",
        "Rearranging gives Bayes Theorem:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "Keep in mind: This is a basic conditional probability derivation.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ How is Bayes' Theorem Used in Classification?\n",
        "\n",
        "In Naive Bayes:\n",
        "\n",
        "$$\n",
        "P(Class|Features) = \\frac{P(Features|Class) \\cdot P(Class)}{P(Features)}\n",
        "$$\n",
        "\n",
        "We compute this probability for each class and pick the one with the **highest score** as the prediction.\n",
        "\n",
        "_Example: Given the words in an email, Naive Bayes calculates how likely it is to be spam._\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Extra Key Points\n",
        "\n",
        "- Works well even with **small datasets**  \n",
        "- **Best choice for text classification** (Bag of Words, TF-IDF)  \n",
        "- **Fast training** (no iterative process)  \n",
        "- **Sensitive to zero probability** ‚Üí Use **Laplace Smoothing**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Quick Summary Table\n",
        "\n",
        "| Concept         | Description                                         |\n",
        "|-----------------|-----------------------------------------------------|\n",
        "| Based on        | Bayes' Theorem                                      |\n",
        "| Assumes         | Feature independence                                |\n",
        "| Used for        | Classification (Spam, Sentiment, Diagnosis, etc.)   |\n",
        "| Type            | Supervised Learning                                 |\n",
        "| Strengths       | Fast, interpretable, handles high-dimensional data  |\n"
      ],
      "metadata": {
        "id": "YkJVHw0IKczM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decision tree"
      ],
      "metadata": {
        "id": "sdwlAzItKoNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ Decision Tree - ML Algorithm (Hinglish Notes )\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What is Decision Tree?\n",
        "\n",
        "Decision Tree ek supervised learning algorithm hai jo **classification** aur **regression** dono tasks ke liye use hoti hai.\n",
        "Yeh algorithm data ko repeatedly split karta hai based on different features, aur ek **tree-like flowchart** banata hai jisme har node pe decision hota hai.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why We Use Decision Tree?\n",
        "\n",
        "* Simple and easy to visualize üß†\n",
        "* Feature scaling ki zarurat nahi (no normalization/standardization)\n",
        "* Numeric + categorical data dono ke liye kaam karta hai\n",
        "* Explainable predictions deta hai (white-box model)\n",
        "* Non-linear problems ke liye bhi kaam karta hai\n",
        "\n",
        "---\n",
        "\n",
        "### üìç When & Where to Use?\n",
        "\n",
        "* Jab human-friendly model chahiye ho\n",
        "* Jab interpretability important ho (e.g. health, finance)\n",
        "* Jab dataset tabular format me ho\n",
        "\n",
        "### üîé Real-Life Examples:\n",
        "\n",
        "* Bank loan approval (age, income, credit score)\n",
        "* Email spam detection (keywords, sender address)\n",
        "* Medical diagnosis (symptoms ke base pe disease predict karna)\n",
        "* Customer churn prediction\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Decision Tree Banane Ka Process:\n",
        "\n",
        "1. **Start with data**\n",
        "2. **Choose the best feature to split** (Entropy & Info Gain ke base par)\n",
        "3. **Create branches** based on feature values\n",
        "4. **Repeat process recursively** for each branch\n",
        "5. **Stop when:**\n",
        "\n",
        "   * All samples belong to the same class (pure data)\n",
        "   * Ya maximum depth reach ho jaye\n",
        "\n",
        "---\n",
        "\n",
        "### üåø Key Concepts:\n",
        "\n",
        "* **Root Node:**\n",
        "  Tree ka sabse pehla node jaha pe first feature-based split hota hai. Yeh feature wo hota hai jiska **Information Gain sabse zyada** hota hai.\n",
        "\n",
        "* **Internal Node:**\n",
        "  Yeh decision nodes hote hain jaha par feature-based split hota hai.\n",
        "\n",
        "* **Leaf Node:**\n",
        "  Final prediction node jaha koi further split nahi hoti. Isme final class ya value aati hai.\n",
        "\n",
        "---\n",
        "\n",
        "### üî• Entropy (Impurity Measure):\n",
        "\n",
        "Entropy batata hai ki ek node me data kitna mixed hai. Pure node (ek hi class) ka entropy 0 hota hai.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "```python\n",
        "Entropy = - Œ£ (p·µ¢ * log‚ÇÇ(p·µ¢))\n",
        "```\n",
        "\n",
        "* yaha `p·µ¢` = class i ka proportion\n",
        "* Entropy 0 ‚Üí pure data (e.g., sab Yes ya sab No)\n",
        "* Entropy high ‚Üí mix of Yes/No\n",
        "\n",
        "üìå **Zyada entropy ‚Üí zyada impurity ‚Üí splitting ka chance zyada**\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Information Gain (Feature selection):\n",
        "\n",
        "Information Gain batata hai ki ek feature use karne se impurity (entropy) kitni kam hoti hai.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "```python\n",
        "Info_Gain = Entropy(Parent) - Weighted Avg Entropy(Children)\n",
        "```\n",
        "\n",
        "‚û°Ô∏è Jiska Information Gain sabse zyada hota hai, us feature ko root node banate hain.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Full Example from Notes (Entropy & Info Gain Calculation):\n",
        "\n",
        "Dataset:\n",
        "\n",
        "| Outlook  | Play |\n",
        "| -------- | ---- |\n",
        "| Sunny    | No   |\n",
        "| Overcast | Yes  |\n",
        "| Rainy    | Yes  |\n",
        "| Sunny    | No   |\n",
        "| Rainy    | Yes  |\n",
        "\n",
        "#### Step 1: Entropy of Target (S)\n",
        "\n",
        "```python\n",
        "Total = 5\n",
        "Yes = 3, No = 2\n",
        "p_yes = 3/5, p_no = 2/5\n",
        "Entropy(S) = - (3/5)*log2(3/5) - (2/5)*log2(2/5)\n",
        "           ‚âà -0.6*log2(0.6) - 0.4*log2(0.4)\n",
        "           ‚âà 0.971\n",
        "```\n",
        "\n",
        "#### Step 2: Outlook ke har value ka entropy calculate karo:\n",
        "\n",
        "* **Sunny:** 2 samples ‚Üí \\[No, No]\n",
        "\n",
        "```python\n",
        "Entropy(Sunny) = 0 (pure class)\n",
        "```\n",
        "\n",
        "* **Overcast:** 1 sample ‚Üí \\[Yes]\n",
        "\n",
        "```python\n",
        "Entropy(Overcast) = 0\n",
        "```\n",
        "\n",
        "* **Rainy:** 2 samples ‚Üí \\[Yes, Yes]\n",
        "\n",
        "```python\n",
        "Entropy(Rainy) = 0\n",
        "```\n",
        "\n",
        "#### Step 3: Weighted Entropy\n",
        "\n",
        "```python\n",
        "Weighted Avg = (2/5)*0 + (1/5)*0 + (2/5)*0 = 0\n",
        "```\n",
        "\n",
        "#### Step 4: Information Gain\n",
        "\n",
        "```python\n",
        "IG(Outlook) = Entropy(S) - Weighted Avg\n",
        "            = 0.971 - 0\n",
        "            = 0.971\n",
        "```\n",
        "\n",
        "‚û°Ô∏è Outlook feature ka Info Gain highest hai ‚Üí ye root node banega\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Repeat Until:\n",
        "\n",
        "* Node me sirf ek hi class ho (pure)\n",
        "* Ya max depth set ho\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ù Decision Tree in Classification & Regression:\n",
        "\n",
        "* **Classification:** Jab output label categorical ho ‚Üí `DecisionTreeClassifier`\n",
        "* **Regression:** Jab output continuous ho ‚Üí `DecisionTreeRegressor`\n",
        "\n",
        "Reason: Tree ke structure me bas splitting logic change hota hai; classifier me entropy/info gain hota hai, regressor me variance reduction.\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Summary:\n",
        "\n",
        "* Entropy ‚Üí impurity measure\n",
        "* Info Gain ‚Üí impurity reduction measure\n",
        "* Feature with highest IG = Root node\n",
        "* Simple, intuitive, explainable\n",
        "* Classification & Regression dono me kaam karta hai\n",
        "* No need for scaling, works on raw data\n",
        "\n",
        "---\n",
        "\n",
        "### üü© Final Line:\n",
        "\n",
        "> **\"Jiska Information Gain sabse zyada hota hai, wahi banega Root Node!\"** üå≥\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "5xzuUkNfirk-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43-nd6sqjKkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíª Support Vector Machine (SVM) - Hinglish Notes for Colab\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What is Support Vector Machine?\n",
        "\n",
        "Support Vector Machine (SVM) ek powerful supervised learning algorithm hai jo mainly **classification** ke liye use hota hai, lekin **regression** ke cases me bhi kaam karta hai (SVR).\n",
        "\n",
        "Yeh algorithm ek **hyperplane** banata hai jo data points ko alag-alag classes me maximum margin ke sath separate karta hai.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why We Use SVM?\n",
        "\n",
        "* Jab data high dimensional ho (e.g., text data)\n",
        "* Jab clear separation ho between classes\n",
        "* Small datasets me bhi achha perform karta hai\n",
        "* Robust to overfitting (especially with right kernel)\n",
        "* Complex problems ke liye nonlinear boundary draw kar sakta hai (via **Kernel trick**)\n",
        "\n",
        "---\n",
        "\n",
        "### üìç When & Where to Use?\n",
        "\n",
        "* Image recognition\n",
        "* Spam email classification\n",
        "* Face detection\n",
        "* Text categorization\n",
        "* Bioinformatics (e.g., cancer classification)\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Basic Concepts (with Definitions):\n",
        "\n",
        "#### üî∏ Hyperplane:\n",
        "\n",
        "* Ek decision boundary hai jo classes ko separate karta hai.\n",
        "* **2D me ek line**, **3D me ek plane**, and **n-D me hyperplane**.\n",
        "\n",
        "#### üî∏ Margin:\n",
        "\n",
        "* Distance between the hyperplane aur closest data points (from both classes).\n",
        "* **Maximum margin = better generalization**\n",
        "\n",
        "#### üî∏ Support Vectors:\n",
        "\n",
        "* Wo points jo hyperplane ke bilkul kareeb hote hain aur decision boundary ko define karte hain.\n",
        "* Inhi points pe model depend karta hai.\n",
        "\n",
        "#### üî∏ Dimension Change:\n",
        "\n",
        "* Jab data linearly separable nahi hota, toh **higher dimension** me transform kar dete hain jaha separation possible ho.\n",
        "\n",
        "#### üî∏ Kernel Trick:\n",
        "\n",
        "* Ye technique high dimension me jaa kar separation ko possible banati hai bina actual me dimension badhaye.\n",
        "* Popular kernels:\n",
        "\n",
        "  * Linear\n",
        "  * Polynomial\n",
        "  * RBF (Gaussian)\n",
        "\n",
        "üß† **We use kernel to distinguish between red and green points when they are not linearly separable.**\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Real-Life Examples:\n",
        "\n",
        "* Email spam vs ham detection\n",
        "* Fraud detection in banking\n",
        "* Face recognition\n",
        "* Disease diagnosis using gene expression\n",
        "* Sentiment analysis\n",
        "\n",
        "---\n",
        "\n",
        "### üìà How SVM Creates the Decision Boundary\n",
        "\n",
        "#### Equation of Hyperplane:\n",
        "\n",
        "```python\n",
        "w¬∑x + b = 0\n",
        "```\n",
        "\n",
        "* `w` ‚Üí weight vector (direction/slope)\n",
        "* `x` ‚Üí input data point\n",
        "* `b` ‚Üí bias term (shifts the hyperplane)\n",
        "\n",
        "**Point above the hyperplane:** w¬∑x + b > 0\n",
        "**Point below the hyperplane:** w¬∑x + b < 0\n",
        "**Point on the hyperplane:** w¬∑x + b = 0\n",
        "\n",
        "‚û°Ô∏è Is equation se pata chalta hai ki koi point kis side of decision boundary me aata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Diagram (Visual Aid for Concept)\n",
        "\n",
        "Yaha diagram ke liye Colab ya markdown me image lagana ideal hota hai, text se confusion ho sakta hai. Isliye ab diagram section hata rahe hain taaki clarity bani rahe.\n",
        "\n",
        "(You can add a clean diagram later using `from IPython.display import Image` in Colab if needed)\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Intuition Recap:\n",
        "\n",
        "* SVM ka goal hota hai: **maximize the margin** between classes\n",
        "* Agar data separable nahi hai: **Kernel trick** se nonlinear boundary banayi jaati hai\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Summary:\n",
        "\n",
        "* SVM supervised learning algorithm hai\n",
        "* Classification + Regression dono ke liye\n",
        "* Linear ya nonlinear problems ke liye\n",
        "* Margin maximize karta hai ‚Üí better generalization\n",
        "* Kernel trick se nonlinear data ko handle karta hai\n",
        "* Small datasets me bhi strong performer\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Final Line:\n",
        "\n",
        "> **\"Support Vector Machine ek boundary banata hai jo classes ko maximum margin ke sath separate karta hai, aur sabse important points hote hain ‚Äî Support Vectors!\"** üöÄ\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "UrqUu3kG1l3k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aC51R1hS1pS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
