{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKnBFT6JAlWP/bmZPDgvAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/SL_Hyperparameter_Tuning_Grid_Random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Model Tuning (Hyperparameter Tuning)\n",
        "\n",
        "Jab bhi hum koi Machine Learning model banate hain, ek baat fix hai:\n",
        "\n",
        "> ‚ö†Ô∏è **First version of the model kabhi best nahi hota.**\n",
        "\n",
        "Isi liye zarurat padti hai **Model Tuning** ki.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Hyperparameters kya hote hain?\n",
        "\n",
        "Model ke kuch settings hote hain jo hum manually set karte hain. Jaise:\n",
        "\n",
        "- KNN ‚Üí `k` (kitne neighbors)\n",
        "- Decision Tree ‚Üí `max_depth`, `min_samples_split`\n",
        "- Gradient Boosting ‚Üí `learning_rate`, `n_estimators`\n",
        "\n",
        "Ye sab cheezein model ka behavior define karti hain.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó Agar randomly choose karein:\n",
        "\n",
        "- Model **underfit** ho sakta hai (kam sikhega)\n",
        "- Ya **overfit** (sirf training yaad karke baaki fail karega)\n",
        "- Ya simply **poor performance** de sakta hai\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Goal of Model Tuning:\n",
        "\n",
        "1. Best hyperparameter ka combination find karna\n",
        "2. Taaki model **new, unseen data** pe bhi achha perform kare (sirf training pe nahi)\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Short Me:\n",
        "\n",
        "> **Model Tuning = Apne model ka full potential nikalna üî•**  \n",
        "> Isse hi **Hyperparameter Tuning** bhi kehte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Next Step:\n",
        "\n",
        "Model tuning start karne se pehle ek important cheez samajhni padti hai:\n",
        "\n",
        "> ‚úÖ **Cross Validation** ‚Äî kya hai aur kaise kaam karta hai\n"
      ],
      "metadata": {
        "id": "lVbeUPXEvYUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In English"
      ],
      "metadata": {
        "id": "e9ri6BB3wBQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Model Tuning (Hyperparameter Tuning)\n",
        "\n",
        "When we build a Machine Learning model, there‚Äôs one big truth:\n",
        "\n",
        "> ‚ö†Ô∏è **The first version of your model is never the best version.**\n",
        "\n",
        "That‚Äôs where **Model Tuning** comes in.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è What are Hyperparameters?\n",
        "\n",
        "These are settings that control how your model behaves. Some examples:\n",
        "\n",
        "- KNN ‚Üí `k` (number of neighbors)\n",
        "- Decision Tree ‚Üí `max_depth`, `min_samples_split`\n",
        "- Gradient Boosting ‚Üí `learning_rate`, `n_estimators`\n",
        "\n",
        "These must be set **before** training starts.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùó If chosen randomly:\n",
        "\n",
        "- Model may **underfit** (learns too little)\n",
        "- Or **overfit** (memorizes training data)\n",
        "- Or just perform **poorly** overall\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Goal of Model Tuning:\n",
        "\n",
        "1. Find the **best combination of hyperparameters**\n",
        "2. So the model performs better on **new, unseen data**\n",
        "\n",
        "---\n",
        "\n",
        "### üí° In Short:\n",
        "\n",
        "> **Model Tuning = Squeezing out the best possible performance from your model üî•**  \n",
        "> This is also known as **Hyperparameter Tuning**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÅ Next Step:\n",
        "\n",
        "Before tuning begins, it‚Äôs important to understand:\n",
        "\n",
        "> ‚úÖ **Cross Validation** ‚Äî what it is and how to use it\n"
      ],
      "metadata": {
        "id": "y8l5LHimwHoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#just a enhanced version of the notes"
      ],
      "metadata": {
        "id": "wAB9IS9DwKFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Model Tuning (Hyperparameter Tuning)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What:\n",
        "Model tuning matlab ‚Äî model ke **hyperparameters** ko optimize karna taaki **best performance** mile.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why:\n",
        "- Kyunki model ke default ya random parameters se output weak ho sakta hai.\n",
        "- Tuning se **underfitting** aur **overfitting** dono control hota hai.\n",
        "- Ye final model ka performance strong banata hai.\n",
        "\n",
        "---\n",
        "\n",
        "### üïí When:\n",
        "- Jab model ban gaya ho aur ab usse aur behtar banana ho.\n",
        "- Jab accuracy low ho ya generalization me dikkat ho.\n",
        "\n",
        "---\n",
        "\n",
        "### üìç Where (Use hota hai):\n",
        "- Har supervised learning model me ‚Äî Logistic Regression, KNN, SVM, Decision Trees, etc.\n",
        "- **Model Deployment** se pehle hamesha tuning karna chahiye.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How:\n",
        "1. Hyperparameters set karte hain (like `k` in KNN, `max_depth` in DT, etc.)\n",
        "2. Alag-alag values try karte hain (GridSearchCV ya RandomSearchCV se)\n",
        "3. Cross Validation lagake test karte hain\n",
        "4. Best value select karke final model banate hain\n",
        "\n",
        "---\n",
        "\n",
        "### üåç Real-life Examples:\n",
        "- Credit card fraud detection ‚Üí best model chahiye jo false alarms kam de\n",
        "- Medical diagnosis ‚Üí hyperparameter tuning se accurate prediction\n",
        "- Email spam filter ‚Üí tuning se precision/recall improve hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### üí° In Short:\n",
        "> **Model Tuning = Apne model ka full potential nikalna üî•**  \n",
        "> Isi ko **Hyperparameter Tuning** bhi kehte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚è≠Ô∏è Next Topic:\n",
        "Pehle Cross Validation samajhna zaroori hai ‚Äî tabhi tuning sahi se ho sakti hai.\n"
      ],
      "metadata": {
        "id": "02tdudXpxStL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hm try karte ha ki best combination mile hyperparameter  unseen data."
      ],
      "metadata": {
        "id": "MhzFAbK8SgV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Model Tuning (Hyperparameter Tuning)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What:\n",
        "Model tuning matlab ‚Äî model ke hyperparameters ko tune karna taaki best version mile.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why:\n",
        "- Random hyperparameters se model **underfit**, **overfit** ya poor perform kar sakta hai.\n",
        "- Isiliye tuning karte hain taaki **model unseen data pe achha perform kare.**\n",
        "\n",
        "---\n",
        "\n",
        "### üïí When:\n",
        "- Jab base model ready ho jaata hai\n",
        "- Tuning hamesha **model finalize karne se pehle** ki jaati hai\n",
        "\n",
        "---\n",
        "\n",
        "### üìç Where:\n",
        "- Sabhi supervised ML models me ‚Äî jaise:\n",
        "  - Decision Tree ‚Üí `max_depth`, `min_samples_split`\n",
        "  - KNN ‚Üí `k` (neighbors ki sankhya)\n",
        "  - Gradient Boost ‚Üí `learning_rate`\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How:\n",
        "- Jab hum KNN jaise model me **different `k` values try karte hain** (jaise 3, 5, 7, 9), toh har value par **accuracy alag hoti hai**.\n",
        "- Tuning ka kaam hai:\n",
        "  1. **Multiple hyperparameter values** try karna\n",
        "  2. **Cross validation** use karke har value ko test karna\n",
        "  3. **Jo best result de**, use final model me rakhna\n",
        "\n",
        "---\n",
        "\n",
        "### üåç Real-life Examples:\n",
        "- Final model ko deploy karne se pehle tuning zaruri hoti hai\n",
        "- Jise ki model **real-world data pe fail na kare**\n"
      ],
      "metadata": {
        "id": "9nI-1LG5T0yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation"
      ],
      "metadata": {
        "id": "IAurj0rUT1Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8D9F2ZFUXy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Cross Validation\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What:\n",
        "Cross Validation ek technique hai jisme hum apne data ko multiple parts me divide karke model ko baar-baar train & test karte hain ‚Äî taaki performance zyada reliable ho.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why:\n",
        "- Kabhi kabhi agar hum **80% train + 20% test** karte hain, toh model usi 20% ke basis pe judge hota hai.\n",
        "- Agar wo 20% lucky ya biased ho gaya, toh model ka score galat lag sakta hai.\n",
        "- Isliye **Cross Validation** karte hain ‚Äî taaki **underfitting/overfitting** detect ho sake aur final performance fair ho.\n",
        "\n",
        "---\n",
        "\n",
        "### üïí When:\n",
        "- Jab data zyada nahi hai\n",
        "- Jab model ka accuracy up-down kar raha ho\n",
        "- Jab best hyperparameters dhoondhne ho (GridSearchCV me use hota hai)\n",
        "\n",
        "---\n",
        "\n",
        "### üìç Where:\n",
        "- Har supervised learning problem me use hota hai: classification ho ya regression\n",
        "- Hyperparameter tuning ke time bhi use hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How:\n",
        "\n",
        "#### üìå Tu jo points bola usko yahan highlight kiya gaya hai:\n",
        "\n",
        "üîπ **Generally, hum 100% data ko 80% training + 20% testing** ke liye use karte hain ‚Äî  \n",
        "But ye method **hamesha sahi result nahi deta**, aur **overfitting / underfitting** ho sakti hai.\n",
        "\n",
        "üîπ Isliye **Cross Validation** me hum **K-Fold Cross Validation** ka use karte hain.\n",
        "\n",
        "üîπ Example: **K = 5**\n",
        "\n",
        "```text\n",
        "DATA  ‚ûù  ‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  (100% data)\n",
        "\n",
        "Split into 5 parts:\n",
        "Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5\n",
        "\n",
        "Step 1: Train on 1+2+3+4 ‚Üí Test on 5  \n",
        "Step 2: Train on 1+2+3+5 ‚Üí Test on 4  \n",
        "Step 3: Train on 1+2+4+5 ‚Üí Test on 3  \n",
        "Step 4: Train on 1+3+4+5 ‚Üí Test on 2  \n",
        "Step 5: Train on 2+3+4+5 ‚Üí Test on 1\n",
        "---\n",
        "\n",
        "### üîÅ Key Points in K-Fold Cross Validation\n",
        "\n",
        "---\n",
        "\n",
        "üîÑ **Har step me testing data ka 20% change hota hai**\n",
        "\n",
        "- Jab `k = 5` hota hai, toh data 5 parts me divide hota hai.\n",
        "- Har round me ek **alag 20% part testing** ke liye use hota hai.\n",
        "- Isse model ko **har section pe test karna possible hota hai**.\n",
        "\n",
        "---\n",
        "\n",
        "üìâ **Har baar model ki accuracy score different ho sakti hai**\n",
        "\n",
        "- Kyunki har test set alag hota hai.\n",
        "- Kabhi test data easy hoga, kabhi tough ‚Äî isliye **performance fluctuate kar sakti hai**.\n",
        "\n",
        "---\n",
        "\n",
        "üìä **End me sab scores ka average nikaalte hain ‚Äì jise bolte hain final model performance**\n",
        "\n",
        "- Sabhi folds ka result ek saath leke:\n",
        "  ```python\n",
        "  final_accuracy = (acc1 + acc2 + acc3 + acc4 + acc5) / 5\n",
        "Ye average score model ka real performance dikhata hai.\n",
        "\n",
        "üåç Real-life Examples\n",
        "üè• Medical Diagnosis:\n",
        "Medical datasets chhote hote hain ‚Äî isliye cross-validation reliable performance deta hai.\n",
        "\n",
        "üè¶ Credit Scoring:\n",
        "Multiple test splits ensure karte hain ki model alag-alag customers pe kaam kare.\n",
        "\n",
        "üéØ Kaggle Competitions:\n",
        "Almost har winning solution me K-Fold CV use hota hai for best accuracy and stability."
      ],
      "metadata": {
        "id": "s1EqN82IWzso"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llvI6GgwXXJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üí° Short Summary:**\n",
        "\n",
        "Cross Validation = Fair testing + Reliable accuracy.\n",
        "\n",
        "K-Fold CV = Model ko har angle se test karna **üîÅ**"
      ],
      "metadata": {
        "id": "Hv9L0x_wXYFN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ec0YJXJ3X7pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéØ Hyperparameter Tuning ‚Äì Based on Your Notes\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What is Hyperparameter Tuning?\n",
        "\n",
        "> Hyperparameter tuning matlab: **model ke external settings** ko test karna  \n",
        "> taaki **best combination mil sake** jo highest accuracy de.\n",
        "\n",
        "Ye parameters **training ke pehle set hote hain**, aur training ke dauraan change nahi hote.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Examples from Notes (Hyperparameters in Models):\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Ridge & Lasso Regression\n",
        "\n",
        "- Regularization models hain jo **overfitting ko control** karte hain  \n",
        "- Hyperparameter: `alpha` (ya `lambda`)  \n",
        "  ‚û§ Ye decide karta hai kitna penalty lagana hai model ke coefficients par\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ KNN (K-Nearest Neighbors)\n",
        "\n",
        "- Hyperparameter: `n_neighbors`  \n",
        "  ‚û§ Jaise `[3, 5, 7, 9]` try karte hain ‚Äî sab par alag accuracy aati hai  \n",
        "  ‚û§ **Isi liye hyperparameter tuning karni padti hai**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Decision Tree\n",
        "\n",
        "- Hyperparameters:\n",
        "  - `max_depth` ‚Üí Kitni depth tak tree jaaye\n",
        "  - `min_samples_split` ‚Üí Node split hone ke liye min sample kitne chahiye\n",
        "  - `max_features` ‚Üí Har node pe consider hone wale features ki sankhya\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Manual Search (Basic Method)\n",
        "\n",
        "### ‚úÖ What:\n",
        "> Jab hum manually hyperparameters change karke accuracy check karte hain\n",
        "\n",
        "### ‚ùó Problem:\n",
        "> Jaise Titanic dataset me:\n",
        "- RandomForest ka `n_estimators = 100` pe accuracy 82.7 thi\n",
        "- Manually `50` set kiya toh gir ke 78.5 ho gayi\n",
        "- **Kab tak aise guess karte rahenge?** üòì\n",
        "\n",
        "üìå **Manual tuning is slow & not scalable**\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Automated Methods for Hyperparameter Tuning\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 1. Grid Search CV\n",
        "\n",
        "#### ‚úÖ What:\n",
        "> Sab possible combinations ko **test karta hai systematically**\n",
        "\n",
        "#### üì¶ Breakdown:\n",
        "| Word        | Meaning |\n",
        "|-------------|---------|\n",
        "| **Grid**    | Table of all possible hyperparameter values  \n",
        "| **Search**  | In sab combinations me se best ko find karna  \n",
        "| **CV**      | Cross Validation ‚Äì har combo ko fair tarike se test karna\n",
        "\n",
        "#### ‚öôÔ∏è How it works:\n",
        "```text\n",
        "Example:\n",
        "n_neighbors = [3, 5, 7]\n",
        "weights = ['uniform', 'distance']\n",
        "\n",
        "Grid:\n",
        "[3, 'uniform']\n",
        "[3, 'distance']\n",
        "[5, 'uniform']\n",
        "[5, 'distance']\n",
        "[7, 'uniform']\n",
        "[7, 'distance']\n",
        "Total = 3 √ó 2 = 6 combinations\n",
        "Har combo par CV apply hoga ‚Üí accuracy milegi ‚Üí best wala choose hoga\n",
        "\n",
        "‚úÖ Guarantee deta hai best combo milega\n",
        "‚ùå But time-consuming hota hai (slow if grid is large)\n",
        "\n",
        "üé≤ 2. Randomized Search CV\n",
        "‚úÖ What:\n",
        "Random combinations try karta hai grid me se ‚Äî time bachane ke liye\n",
        "\n",
        "‚öôÔ∏è How:\n",
        "Grid define karte hain, but sabhi try nahi karta\n",
        "\n",
        "Random k combinations pick karta hai (jaise 10 out of 100)\n",
        "\n",
        "Fast hota hai, especially jab grid bada ho\n",
        "\n",
        "‚úÖ Faster than GridSearchCV\n",
        "‚ùå Might miss the absolute best combination (kyunki sab try nahi karta)\n",
        "\n",
        "üåç Real-life Examples:\n",
        "üéØ Kaggle models ‚Üí mostly GridSearchCV use hota hai\n",
        "\n",
        "üè• Healthcare me accuracy critical hoti hai ‚Äî tuning must\n",
        "\n",
        "üìä Titanic dataset ‚Üí Manual tuning se pata chalta hai ki automated best hai\n",
        "---\n",
        "\n",
        "### üí° Summary: Hyperparameter Tuning Methods\n",
        "\n",
        "| Method               | Speed    | Accuracy     | Use Case              |\n",
        "|----------------------|----------|--------------|------------------------|\n",
        "| Manual Search        | üêå Slow   | ‚ùå Unreliable | Basic trial & error   |\n",
        "| GridSearchCV         | ‚öñÔ∏è Medium | ‚úÖ Best       | Small search space     |\n",
        "| RandomizedSearchCV   | ‚ö° Fast   | üîÑ Good       | Large search space     |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PHQwLRn4fWaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#randomized search"
      ],
      "metadata": {
        "id": "Y_8KjU79f2Ga"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üé≤ RandomizedSearchCV ‚Äì Hyperparameter Tuning (Smart + Fast)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What:\n",
        "RandomizedSearchCV ek technique hai jisme:\n",
        "- Hum **poori grid ki jagah**, **randomly kuch combinations** ko hi try karte hain.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì Why:\n",
        "- Jab parameter combinations **bohot zyada ho jayein** (100s or 1000s)\n",
        "- Tab GridSearchCV **slow aur expensive** ho jaata hai\n",
        "- RandomizedSearchCV se time save hota hai, fir bhi **achha result milta hai**\n",
        "\n",
        "---\n",
        "\n",
        "### üïí When to use:\n",
        "- Jab model slow ho (jaise XGBoost)\n",
        "- Jab grid size bohot bada ho\n",
        "- Jab quick but fairly good result chahiye ho\n",
        "\n",
        "---\n",
        "\n",
        "### üìç Where used:\n",
        "- Large models like RandomForest, GradientBoost, XGBoost\n",
        "- Kaggle competitions for quick tuning\n",
        "- Real-time ML pipelines jaha tuning me time kam hota hai\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How it works:\n",
        "\n",
        "- Grid define karte ho jaise:\n",
        "  ```python\n",
        "  param_grid = {\n",
        "      'n_estimators': [50, 100, 200, 300],\n",
        "      'max_depth': [3, 5, 10, 20],\n",
        "      'min_samples_split': [2, 5, 10]\n",
        "  }\n",
        "---\n",
        "\n",
        "### üí° Summary: Hyperparameter Tuning Methods\n",
        "\n",
        "| Method               | üîÅ Speed   | üéØ Accuracy     | üìå Use Case              |\n",
        "|----------------------|------------|------------------|---------------------------|\n",
        "| Manual Search        | üêå Slow     | ‚ùå Unreliable     | Basic trial & error       |\n",
        "| GridSearchCV         | ‚öñÔ∏è Medium   | ‚úÖ Best           | Small search space        |\n",
        "| RandomizedSearchCV   | ‚ö° Fast     | üîÑ Good           | Large search space        |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DFyQxid6f6cW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BSHeaxH9f4yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### üé≤ RandomizedSearchCV ‚Äì Smart Tuning for Large Search Spaces\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Problem: Jab hyperparameter combinations bohot zyada ho\n",
        "\n",
        "> Jab chhoti grid hoti hai (jaise 5‚Äì10 combinations), toh GridSearchCV handle kar sakta hai.\n",
        "\n",
        "‚ùó But suppose XGBoost ke case me:\n",
        "- `n_estimators` = [100, 200, ..., 1000]\n",
        "- `max_depth` = [3, 5, 7, 10]\n",
        "- `learning_rate` = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "Total combinations = **10 √ó 4 √ó 4 = 160**  \n",
        "Aur agar aur zyada params ho gaye (2000‚Äì3000), toh GridSearch **bohot slow ho jaata hai**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Solution: RandomizedSearchCV\n",
        "\n",
        "- Ye **poore grid ko scan nahi karta**\n",
        "- Sirf randomly **kuch combinations (jaise 10 ya 20)** pick karta hai\n",
        "- Time bachaata hai, fir bhi **achi accuracy mil jaati hai**\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ `n_estimators` kya hota hai?\n",
        "\n",
        "> Ye decide karta hai ki **kitne trees banaye jaayenge** model me (mostly in ensemble models like RandomForest, XGBoost)\n",
        "\n",
        "- Zyadatar case me `n_estimators` = 100 ya 200 rakhte hain\n",
        "- Zyada trees = zyada training time, lekin better learning (up to a point)\n",
        "\n",
        "‚úÖ RandomizedSearchCV me hum is value ko **efficiently test kar sakte hain**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Short Example:\n",
        "\n",
        "```python\n",
        "param_grid = {\n",
        "  'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
        "  'max_depth': [3, 5, 7, 10],\n",
        "  'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "\n",
        "random_cv = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=20, cv=5)\n",
        "random_cv.fit(X, y)\n"
      ],
      "metadata": {
        "id": "d8CDOu-36md1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxReG0qI6nC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
