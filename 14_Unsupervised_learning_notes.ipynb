{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy20+jeO9qoGybU0dIdc7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/Machine-Learning/blob/main/14_Unsupervised_learning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NErMZGupTBda"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Unsupervised Learning (USL) – Hidden Intelligence 🧩\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What is Unsupervised Learning?\n",
        "\n",
        "Unsupervised Learning ek aisi Machine Learning technique hai jisme:\n",
        "- **Data ke sath koi label nahi hota**\n",
        "- Model ko **khud patterns aur structure dhoondhne padte hain**\n",
        "- Output pre-defined nahi hota → model khud clusters / groups / rules nikalta hai\n",
        "\n",
        "---\n",
        "\n",
        "## ❓ Why do we use Unsupervised Learning?\n",
        "\n",
        "- Jab:\n",
        "  - **Labeling data mushkil ya expensive ho**\n",
        "  - Hume **unknown patterns / insights** discover karne ho\n",
        "  - Hume data ko group ya simplify karna ho\n",
        "\n",
        "🎯 **Goal**:\n",
        "- Pattern find karna  \n",
        "- Hidden structure samajhna  \n",
        "- Clustering, Dimensionality Reduction, Outlier Detection karna\n",
        "\n",
        "---\n",
        "\n",
        "## 🕐 When to Use?\n",
        "\n",
        "- Jab data me labels available nahi hote  \n",
        "- Jab data high-dimensional ho  \n",
        "- Jab objective sirf pattern samajhna ho, prediction nahi\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Where to Use?\n",
        "\n",
        "| Use Case                 | Example                                 |\n",
        "|--------------------------|-----------------------------------------|\n",
        "| 🏦 Bank Fraud Detection  | Outlier Detection                       |\n",
        "| 🛍️ Customer Segmentation | Clustering (market analysis)            |\n",
        "| 📷 Image Compression      | Dimensionality Reduction (PCA)          |\n",
        "| 🎧 Music Recommendation   | Find user taste clusters                |\n",
        "| 🧬 Medical Research       | Patient grouping via gene data          |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ How it Works?\n",
        "\n",
        "1. Input = Unlabeled data  \n",
        "2. Algorithm = Clustering / PCA / DBSCAN / Anomaly Detection  \n",
        "3. Output = Clusters, compressed features, outlier flags  \n",
        "4. No predefined \"right\" answer → just hidden structure discovery\n",
        "\n",
        "---\n",
        "\n",
        "## 🌟 Real-Life Examples:\n",
        "\n",
        "| Problem                      | USL Technique        |\n",
        "|------------------------------|----------------------|\n",
        "| Bank Fraud Detection         | Outlier Detection    |\n",
        "| Customer Segmentation        | Clustering (K-Means) |\n",
        "| Image Compression            | PCA                  |\n",
        "| Student Grouping by Behavior | Clustering           |\n",
        "| Noise Removal in Data        | PCA / ICA            |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Supervised vs Unsupervised Learning\n",
        "\n",
        "| Feature               | Supervised Learning             | Unsupervised Learning                |\n",
        "|-----------------------|----------------------------------|--------------------------------------|\n",
        "| Input                 | Features + Labels                | Only Features                        |\n",
        "| Output                | Predefined (Yes/No, Price)       | Discovered (clusters/patterns)       |\n",
        "| Goal                  | Prediction / Classification      | Pattern Discovery / Grouping         |\n",
        "| Examples              | LR, DT, SVM, KNN                 | K-Means, PCA, DBSCAN, IsolationForest|\n",
        "| Use Cases             | Spam detection, Price prediction | Customer segmentation, Fraud detect  |\n",
        "| From Notes            | Classification / Regression      | Clustering / Dimensionality Reduction|\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Key Use Goals\n",
        "\n",
        "- ✔️ Pattern find karna  \n",
        "- ✔️ Hidden structure identify karna  \n",
        "- ✔️ Clustering (group similar data)  \n",
        "- ✔️ Dimensionality Reduction (DR)  \n",
        "- ✔️ Outlier Detection (fraud detection)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧬 Types of Unsupervised Learning\n",
        "\n",
        "### 1️⃣ Clustering\n",
        "> Similar data points ko group karna (no labels)\n",
        "\n",
        "**Popular Clustering Algorithms:**\n",
        "- 🔹 K-Means\n",
        "- 🔹 Hierarchical Clustering\n",
        "- 🔹 DBSCAN (Density-Based)\n",
        "\n",
        "### 2️⃣ Dimensionality Reduction (DR)\n",
        "> High-dimensional data ko kam features me convert karna while preserving structure\n",
        "\n",
        "**Common DR Techniques:**\n",
        "- 🔸 PCA (Principal Component Analysis)\n",
        "- 🔸 t-SNE\n",
        "- 🔸 Autoencoders\n",
        "- 🔸 ICA (Independent Component Analysis)\n",
        "\n",
        "### 3️⃣ Anomaly / Outlier Detection\n",
        "> Unusual ya rare data points identify karna\n",
        "\n",
        "**Popular Techniques:**\n",
        "- 🔺 Isolation Forest\n",
        "- 🔺 One-Class SVM\n",
        "- 🔺 DBSCAN (outlier as noise)\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Memory Table (Clean Summary)\n",
        "\n",
        "| Type                     | Sub-Types / Algorithms                   | Goal                               |\n",
        "|--------------------------|------------------------------------------|------------------------------------|\n",
        "| 🔵 Clustering            | K-Means, DBSCAN, Hierarchical            | Group similar points               |\n",
        "| 🔶 Dimensionality Reduction | PCA, t-SNE, ICA, Autoencoders           | Reduce data dimensions             |\n",
        "| 🔺 Outlier Detection     | Isolation Forest, One-Class SVM, DBSCAN  | Detect rare / unusual points       |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mXWkWXlXXplt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNZuKSSHXqLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧰 Common Techniques in Unsupervised Learning\n",
        "\n",
        "---\n",
        "\n",
        "## 1️⃣ Clustering – Similar Data Points Ko Group Karna\n",
        "\n",
        "### 📌 What it does:\n",
        "- Similar data points ko **group** karta hai\n",
        "- Har group = **cluster**\n",
        "- No labels, only data patterns\n",
        "\n",
        "### 📈 Algorithms:\n",
        "- **K-Means**\n",
        "- **Hierarchical Clustering**\n",
        "- **DBSCAN**\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 🛍️ Customer Segmentation → Grouping customers by purchase behavior\n",
        "- 🎧 Music App → Grouping songs by user listening pattern\n",
        "- 🧬 Medical → Patient disease pattern clustering\n",
        "\n",
        "---\n",
        "\n",
        "## 2️⃣ Dimensionality Reduction (DR)\n",
        "\n",
        "### 📌 What it does:\n",
        "- **High-dimensional** data ko **low dimensions** me convert karta hai\n",
        "- Core idea: **important info ko preserve karna**, useless noise hata dena\n",
        "\n",
        "### 📈 Algorithms:\n",
        "- **PCA (Principal Component Analysis)**\n",
        "- **t-SNE (Visualization)**\n",
        "- **ICA (Independent Component Analysis)**\n",
        "- **Autoencoders (Neural Network-based DR)**\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 📷 Image Compression → Reduce size using PCA\n",
        "- 📊 Visualization → Plot high-dimensional data in 2D/3D using t-SNE\n",
        "- 🧬 Genomics → Reduce 10,000 gene columns → 100 key signals\n",
        "\n",
        "---\n",
        "\n",
        "## 3️⃣ Anomaly Detection / Outlier Detection\n",
        "\n",
        "### 📌 What it does:\n",
        "- **Rare / abnormal patterns** detect karta hai\n",
        "- Normal data se **alagalag** hone wale points ko flag karta hai\n",
        "\n",
        "### 📈 Algorithms:\n",
        "- **Isolation Forest**\n",
        "- **One-Class SVM**\n",
        "- **DBSCAN (noise points)**\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 🏦 Fraud Detection → Unusual transaction pattern detect\n",
        "- 🌐 Network Security → Intrusion detection system\n",
        "- 🏥 Health → Abnormal heartbeats detection (ECG)\n",
        "\n",
        "---\n",
        "\n",
        "## 4️⃣ Association Rule Learning\n",
        "\n",
        "### 📌 What it does:\n",
        "- Data ke items ke **co-occurrence** ya association dhoondhta hai\n",
        "\n",
        "### 📈 Algorithm:\n",
        "- **Apriori Algorithm**\n",
        "- **Eclat Algorithm**\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 🛒 Market Basket Analysis → \"Customers who buy bread also buy butter\"\n",
        "- 📈 Cross-Selling → Suggest related products\n",
        "\n",
        "---\n",
        "\n",
        "## 5️⃣ Autoencoders – Deep Learning-based DR\n",
        "\n",
        "### 📌 What it does:\n",
        "- Data compress karta hai and reconstruct karta hai using neural networks\n",
        "- Use hota hai **dimensionality reduction** ya **noise removal** me\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 🖼️ Denoising images (remove blur or noise)\n",
        "- 📄 Document encoding (semantic compression)\n",
        "- 📦 Feature compression before supervised learning\n",
        "\n",
        "---\n",
        "\n",
        "## 6️⃣ Visualization Techniques\n",
        "\n",
        "### 📌 What it does:\n",
        "- High-dimensional data ko **2D/3D** plots me convert karta hai\n",
        "- Human-friendly representation deta hai\n",
        "\n",
        "### 📈 Tools:\n",
        "- **t-SNE**\n",
        "- **UMAP**\n",
        "- **PCA (for visualizing clusters)**\n",
        "\n",
        "### 💡 Real-Life Examples:\n",
        "- 📊 Customer clusters ko visualize karna\n",
        "- 🧬 Visualizing gene groupings\n",
        "- 🧠 AI model behavior ko understand karna\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary Table\n",
        "\n",
        "| Technique              | Purpose                          | Example Use Case                   |\n",
        "|------------------------|----------------------------------|------------------------------------|\n",
        "| Clustering             | Group similar data points        | Customer segmentation              |\n",
        "| Dimensionality Reduction | Reduce features, remove noise   | Image compression, gene signals    |\n",
        "| Anomaly Detection      | Find unusual points              | Fraud detection, health anomalies  |\n",
        "| Association Rules      | Discover item relationships      | Market basket analysis             |\n",
        "| Autoencoders           | Neural DR / noise removal        | Denoising images, compress text    |\n",
        "| Visualization          | Visualize high-dim data          | Cluster plots, model explanation   |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9vp7RFeTZNxV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFiupUvPZRKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Clustering – Grouping Similar Data (Unsupervised Learning)\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What is Clustering?\n",
        "\n",
        "**Clustering** ek Unsupervised Learning technique hai jisme:\n",
        "- Similar data points ko ek group (cluster) me divide kiya jaata hai  \n",
        "- Labels nahi hote → model khud patterns find karta hai  \n",
        "- Har cluster me items **internally similar** hote hain aur **dusre clusters se different**\n",
        "\n",
        "---\n",
        "\n",
        "## ❓ Why Do We Use Clustering?\n",
        "\n",
        "- Jab hume:\n",
        "  - Large unlabeled data me hidden patterns dhoondhne ho\n",
        "  - Similar user types ya behavior samajhna ho\n",
        "  - Natural groups banane ho for better decision making\n",
        "\n",
        "🎯 Goal: **Find structure & pattern without any supervision**\n",
        "\n",
        "---\n",
        "\n",
        "## 🕐 When Do We Use Clustering?\n",
        "\n",
        "- Jab:\n",
        "  - Data unlabeled ho\n",
        "  - Groups pehle se defined na ho\n",
        "  - Hum **exploratory data analysis** (EDA) kar rahe ho\n",
        "  - Market segmentation / recommendation / grouping chahiye ho\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Where Do We Use Clustering?\n",
        "\n",
        "| Use Case                  | Example                                        |\n",
        "|---------------------------|------------------------------------------------|\n",
        "| 🛍️ Marketing              | Customer segmentation (grouping by behavior)   |\n",
        "| 🧬 Medical Research        | Grouping diseases/patients by symptom pattern |\n",
        "| 🎧 Music App              | Grouping songs by genre similarity             |\n",
        "| 📷 Image Compression      | Divide similar pixels → compress               |\n",
        "| 🧠 Social Media Analysis  | Group similar posts/users                      |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ How Clustering Works?\n",
        "\n",
        "### Steps:\n",
        "1. Input data bina labels ke diya jaata hai\n",
        "2. Algorithm **data points ke similarity** ko check karta hai\n",
        "3. Based on distance (e.g., Euclidean), points ko groups me divide karta hai\n",
        "4. Final output = Multiple clusters jisme similar items grouped hote hain\n",
        "\n",
        "---\n",
        "\n",
        "## 🤔 Why is it called “Clustering”?\n",
        "\n",
        "- Kyunki:\n",
        "  - Data points **naturally ek jagah grouped hote hain**\n",
        "  - Alag-alag clusters visually bhi **clearly separated** dikhte hain\n",
        "  - Jaise human eye bhi easily **dekh sakti hai** ki kaunsa point kis cluster me belong karta hai (check diagram👇)\n",
        "\n",
        "  \n",
        "  ## 📛 Clustering Ke Naam Ka Reason\n",
        "\n",
        "- Data apne-aap naturally **clusters (groups)** banata hai\n",
        "- In groups ke andar:\n",
        "  - Data points ek dusre ke **kaafi close / similar** hote hain\n",
        "  - Aur doosre clusters se clearly **alag / distant**\n",
        "- In clusters ko hum **naked eye se bhi easily identify** kar sakte hain (e.g., scatter plot)\n",
        "\n",
        "---\n",
        "\n",
        "## 🌟 Real-Life Examples of Clustering\n",
        "\n",
        "| 🧪 Scenario               | 🧩 Cluster Meaning                            |\n",
        "|--------------------------|-----------------------------------------------|\n",
        "| 🏦 Bank Customers         | Grouped by spending behavior                 |\n",
        "| 🛒 E-commerce Users       | Grouped by product preferences               |\n",
        "| 📷 Image Pixels           | Grouped by similar pixel colors              |\n",
        "| 🧠 Psychological Testing  | Grouped by thinking patterns                 |\n",
        "| 🎓 Education Analytics    | Grouped by learning styles\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🧊 Diagram (ASCII Style - 2D Visualization)\n",
        "\n",
        "```text\n",
        "       Cluster 1        Cluster 2         Cluster 3\n",
        "        ▓▓▓▓▓             ░░░░░              █████\n",
        "       ▓     ▓           ░     ░            █     █\n",
        "      ▓       ▓         ░       ░          █       █\n",
        "       ▓     ▓           ░     ░            █     █\n",
        "        ▓▓▓▓▓             ░░░░░              █████\n",
        "\n",
        "       (Tightly grouped → visually separable)\n",
        "\n"
      ],
      "metadata": {
        "id": "8KHZGj2iaw0c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPLMnHEzaxwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 Types of Clustering Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 1. K-Means Clustering\n",
        "\n",
        "- **Centroid-based** method\n",
        "- Points are assigned to the cluster with the nearest **mean (centroid)**\n",
        "- Works well for **numeric & continuous data**\n",
        "- Fast & scalable, but sensitive to outliers\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 2. K-Medoids Clustering (PAM)\n",
        "\n",
        "- Similar to K-Means but uses **medoid** (most central data point) instead of mean\n",
        "- More **robust to outliers**\n",
        "- Works better for small datasets or when data has noise\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 3. K-Modes Clustering\n",
        "\n",
        "- Used for **categorical data**\n",
        "- Uses **mode (most frequent value)** instead of mean\n",
        "- Distance is calculated using **Hamming distance**\n",
        "- Useful for clustering strings or categories\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 4. Hierarchical Clustering\n",
        "\n",
        "- Builds a **tree of clusters** (dendrogram)\n",
        "- Two types:\n",
        "  - **Agglomerative (Bottom-Up)**\n",
        "  - **Divisive (Top-Down)**\n",
        "- Doesn’t require pre-deciding number of clusters\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 5. DBSCAN (Density-Based Spatial Clustering)\n",
        "\n",
        "- Clusters based on **density of data points**\n",
        "- Can find **arbitrary shaped clusters**\n",
        "- Automatically detects **outliers/noise**\n",
        "- Good for **non-spherical** clusters\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 6. Mean Shift Clustering\n",
        "\n",
        "- Moves data points toward **high-density regions**\n",
        "- Doesn’t require K value\n",
        "- Good for detecting **blobs** in image processing\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 7. Gaussian Mixture Model (GMM)\n",
        "\n",
        "- **Probabilistic model** using multiple Gaussians\n",
        "- One point can belong to **multiple clusters** with probability\n",
        "- Useful when clusters **overlap**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 8. Spectral Clustering\n",
        "\n",
        "- Uses **graph theory** and eigenvalues of similarity matrix\n",
        "- Good for **non-convex or complex shapes**\n",
        "- Performs well with small-medium datasets\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 9. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\n",
        "\n",
        "- Efficient for **very large datasets**\n",
        "- Uses **clustering features tree (CF Tree)** for scalable clustering\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary Table\n",
        "\n",
        "| Algorithm         | Data Type       | Shape         | K Needed? | Outlier Robust | Speed  |\n",
        "|------------------|------------------|---------------|-----------|----------------|--------|\n",
        "| K-Means           | Numeric           | Spherical     | ✅ Yes     | ❌ No           | ⚡ Fast |\n",
        "| K-Medoids         | Numeric/Mixed     | Any           | ✅ Yes     | ✅ Yes          | 🐢 Slow |\n",
        "| K-Modes           | Categorical       | N/A           | ✅ Yes     | ✅ Yes          | ⚡ Fast |\n",
        "| Hierarchical      | Any               | Any           | ❌ No      | ❌ Partial      | 🐢 Slow |\n",
        "| DBSCAN            | Any               | Arbitrary     | ❌ No      | ✅ Yes          | ⚡ Fast |\n",
        "| Mean Shift        | Any               | Any           | ❌ No      | ✅ Yes          | 🐢 Slow |\n",
        "| GMM               | Numeric           | Overlapping   | ✅ Yes     | ❌ No           | ⚡ Fast |\n",
        "| Spectral          | Any               | Complex       | ✅ Yes     | ❌ No           | ⚡ Mid  |\n",
        "| BIRCH             | Large datasets    | Hierarchical  | ✅ Yes     | ✅ Yes          | ⚡ Fast |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0dZgu17Gu3xb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJB_RtKEu3ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📍 K-Means Clustering – Grouping with Geometry 🎯\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What is K-Means Clustering?\n",
        "\n",
        "**K-Means** ek **unsupervised machine learning** algorithm hai jo:\n",
        "- Similar data points ko **K clusters** me divide karta hai  \n",
        "- Har cluster ka ek **center point (centroid)** hota hai  \n",
        "- Algorithm data points ko nearest centroid ke according assign karta hai\n",
        "\n",
        "---\n",
        "\n",
        "## ❓ Why Use K-Means?\n",
        "\n",
        "- Jab hume:\n",
        "  - Data me **natural groups** dhoondhne ho\n",
        "  - Labeled data na ho\n",
        "  - Clustering fast aur scalable way me karni ho\n",
        "\n",
        "🎯 Goal: Similar data points ko ek group me daalna based on **distance from center**\n",
        "\n",
        "---\n",
        "\n",
        "## 🕐 When to Use K-Means?\n",
        "\n",
        "- Jab:\n",
        "  - Hume large dataset ko **unsupervised way me group** karna ho\n",
        "  - Hume cluster banana ho but labels na mile\n",
        "  - Fast aur easy clustering chahiye\n",
        "\n",
        "---\n",
        "\n",
        "## 🌍 Where to Use K-Means?\n",
        "\n",
        "| Use Case                | Cluster Meaning                              |\n",
        "|-------------------------|-----------------------------------------------|\n",
        "| 🏦 Customer Segmentation | Alag-alag type ke customer group karna       |\n",
        "| 📷 Image Compression     | Similar color pixels cluster karna           |\n",
        "| 🧠 Behavioral Segmentation | Users ko behavior ke base pe group karna    |\n",
        "| 📍 Geo-Location Data     | Cities/stores ko area-wise group karna       |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ How K-Means Works? – Step by Step 💡\n",
        "\n",
        "### 🔢 Step 1: Decide K (Number of Clusters)\n",
        "- Pehle decide karo **kitne clusters** chahiye (K)\n",
        "- Ye data ka nature ya **Elbow Method** se decide kiya ja sakta hai\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Step 2: Initialize Centroids\n",
        "- Randomly **K points** ko initial **centroids** banate hain\n",
        "- 📌 **Centroid** = Cluster ka center (mean position of all points)\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Step 3: Assign Each Data Point to Nearest Centroid\n",
        "- Har point ka **Euclidean distance** sabhi centroids se calculate hota hai:\n",
        "  (\n",
        "Euclidean Distance = √[(x2 - x1)² + (y2 - y1)²])\n",
        "\n",
        "  Jis centroid se distance minimum ho → wohi cluster assign hota hai\n",
        "\n",
        "  ## 🔄 Step 4: Recalculate New Centroid\n",
        "\n",
        "- Har cluster ke sabhi points ka **average (mean)** nikalte hain  \n",
        "- Ye average point hi **cluster ka naya centroid** banta hai\n",
        "\n",
        "---\n",
        "\n",
        "## ♻️ Step 5: Reassign Points Based on New Centroids\n",
        "\n",
        "- Ab sabhi points ka **distance naye centroids** se dobara calculate kiya jata hai  \n",
        "- Har point ko **closest centroid ke cluster** me reassign karte hain\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Step 6: Repeat Until Convergence\n",
        "\n",
        "- Ye process tab tak **repeat** hoti hai jab tak:\n",
        "  - Points ka cluster **change hona band** ho jaye\n",
        "  - Clusters **stable** ho jayein\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Step 7: Final Clusters Achieved\n",
        "\n",
        "- Jab clusters **finalize** ho jaate hain, algorithm **stop** kar deta hai  \n",
        "- Final clusters ka use kiya jata hai:\n",
        "\n",
        "| 💡 Use Case           | 📌 Purpose                      |\n",
        "|-----------------------|---------------------------------|\n",
        "| 📈 Visualization       | Cluster plots draw karne ke liye |\n",
        "| 📊 Grouping            | Natural grouping of data        |\n",
        "| 📦 Feature Engineering | New features banane ke liye     |\n",
        "| 🔍 Recommendation      | User/item similarity find karne ke liye |\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Elbow Method – Best K Choose Karne Ke Liye\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 What is Elbow Method?\n",
        "\n",
        "**Elbow Method** ek technique hai jisme:\n",
        "- Multiple values of **K (clusters)** ke liye model banaya jaata hai\n",
        "- Har K ke liye calculate karte hain:\n",
        "\n",
        "> **WCSS = Within-Cluster Sum of Squares**\n",
        "\n",
        "---\n",
        "\n",
        "### 📉 WCSS Explained:\n",
        "\n",
        "> WCSS = Har data point ka apne centroid se **distance squared ka sum**\n",
        "\n",
        "✅ **Low WCSS** → Clusters are **tight and compact**  \n",
        "❗ **Zyada clusters** → WCSS automatically kam ho jaata hai (overfitting risk)\n",
        "\n",
        "---\n",
        "\n",
        "### 💪 Elbow Curve Logic:\n",
        "\n",
        "- Jab **K increase** karte hain → WCSS continuously decrease hota hai\n",
        "- Ek point aata hai jahan graph **suddenly flat** ho jaata hai  \n",
        "- 📍 Isi point ko kehte hain **\"Elbow Point\"**\n",
        "\n",
        "🧠 **Elbow Point = Optimal K value**  \n",
        "👉 Jahan tak WCSS sharply girta hai, uske baad flat ho jata hai\n",
        "\n",
        "🧠 Real-Life Examples of K-Means:\n",
        "Area\tUse Case\n",
        "🏪 Retail\tCustomer segmentation for marketing\n",
        "🖼️ Image Processing\tCompress images by clustering pixels\n",
        "🧬 Biology\tGrouping similar DNA sequences\n",
        "📍 Maps / Geo Data\tGrouping cities/stores area-wise\n",
        "🧠 Education\tGrouping students by learning style\n",
        "\n",
        "---\n",
        "\n",
        "            Cluster 1            Cluster 2           Cluster 3\n",
        "              🔴                     🔷                   🟢\n",
        "           ●  ●  ●               ●  ●  ●              ●  ●  ●\n",
        "           ● 🔴 ●               ● 🔷 ●              ● 🟢 ●\n",
        "           ●  ●  ●               ●  ●  ●              ●  ●  ●\n",
        "\n",
        "   (Each color = different cluster; center = centroid)\n",
        "\n",
        "📉 Elbow Curve Example:\n",
        "\n",
        "        |\n",
        "     WCSS|\n",
        "        |\\\n",
        "        | \\\n",
        "        |  \\\n",
        "        |   \\\n",
        "        |    \\______\n",
        "        |          |\n",
        "        |__________|_____________\n",
        "                  K (no. of clusters)\n",
        "                 (elbow point)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdYUaYU5ssUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❌ Disadvantages of K-Means Clustering\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ❗ Must Predefine K\n",
        "\n",
        "- Pehle se hi **clusters ki count (K)** decide karni padti hai  \n",
        "- Galat K value → **incorrect clustering**\n",
        "\n",
        "---\n",
        "\n",
        "## 2. ❌ Sensitive to Initial Centroids\n",
        "\n",
        "- Agar starting centroids **poorly choose** kiye gaye  \n",
        "  → algorithm **wrong clusters** detect kar sakta hai\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🔁 Can Converge to Local Minima\n",
        "\n",
        "- K-Means **local minima** me fas sakta hai  \n",
        "- Har bar **same result** nahi deta (due to random init)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 🔥 ❌ **Fails on Circular / Irregular Cluster Shapes**\n",
        "\n",
        "- K-Means **sirf spherical (round) clusters** ke liye kaam karta hai  \n",
        "- **Non-circular / complex shapes** (e.g., moons, spirals) me **bad results** deta hai  \n",
        "- Aise cases me **DBSCAN** ya **Spectral Clustering** better hote hain\n",
        "\n",
        "📌 Example: Moon-shaped ya concentric circular clusters = ❌ Not handled well by K-Means\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 🎯 Only Works with Numeric Data\n",
        "\n",
        "- Sirf **numerical/continuous** features ke saath kaam karta hai  \n",
        "- **Categorical data** ke liye suitable nahi (K-Modes better hai)\n",
        "\n",
        "---\n",
        "\n",
        "## 6. ❌ Not Robust to Outliers\n",
        "\n",
        "- **Outliers** centroids ko **distort** kar dete hain  \n",
        "- Poor clustering ho sakta hai\n",
        "\n",
        "---\n",
        "\n",
        "## 7. ⚠️ Unequal Cluster Sizes\n",
        "\n",
        "- Agar clusters ka size **unequal** hai (e.g., ek bada, ek chhota)\n",
        "  → K-Means **bias** show karta hai\n",
        "\n",
        "---\n",
        "\n",
        "## 8. ⛔ Non-Convex Clusters = Bad Fit\n",
        "\n",
        "- Complex patterns (e.g., C-shape, spiral) ko K-Means nahi samajh pata  \n",
        "- Algorithm **incorrect boundaries** draw karta hai\n",
        "\n",
        "---\n",
        "\n",
        "📌 In short:\n",
        "\n",
        "| Limitation                 | Impact                                       |\n",
        "|----------------------------|----------------------------------------------|\n",
        "| Predefined K               | Needs domain knowledge                      |\n",
        "| Random Init                | Unstable results                             |\n",
        "| Numeric-only               | Categorical data not handled                |\n",
        "| Outliers                   | Bad centroids & cluster assignment          |\n",
        "| 🔥 Non-circular Clusters   | Fails on complex cluster shapes             |\n",
        "\n",
        "---\n",
        "\n",
        "📉 Use K-Means only when:\n",
        "- Data is **numeric**\n",
        "- Clusters are **roughly equal & spherical**\n",
        "- Outliers are **handled/removed**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "54aPmMqv1F22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#kmean centroid based hai and iske disadvantage ko overcome karne ke liye hm DBSCAN ka use karte hai"
      ],
      "metadata": {
        "id": "MBMciGvIuhR3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NurrrTtS1jxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 DBSCAN – Density-Based Spatial Clustering of Applications with Noise\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 What is DBSCAN?\n",
        "\n",
        "**DBSCAN** ek **unsupervised clustering algorithm** hai jo:\n",
        "- Data points ko **density ke base pe clusters** me group karta hai\n",
        "- Low-density points ko **outliers/noise** treat karta hai\n",
        "- Full form: **Density-Based Spatial Clustering of Applications with Noise**\n",
        "\n",
        "---\n",
        "\n",
        "## ❓ Why Use DBSCAN?\n",
        "\n",
        "- **No need to specify K** (number of clusters) → ❌ No pre-defined clusters\n",
        "- Can handle **arbitrary shape clusters** (circular, moon-shape, etc.)\n",
        "- Automatically detects **outliers/noise**\n",
        "- Works well on **spatial/geographical data**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 DBSCAN is Non-Parametric\n",
        "\n",
        "- **Non-parametric** ka matlab: Isme **model parameters ko fix karna zaroori nahi**\n",
        "- Doesn’t assume data is distributed in any fixed shape (e.g., spherical)\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ How DBSCAN Works? – Step-by-Step 🔄\n",
        "\n",
        "### 🔹 Step 1: Choose 2 Parameters\n",
        "- **Epsilon (ε)** → Radius of the neighborhood (distance threshold)\n",
        "- **MinPts** → Minimum number of points needed in ε-radius to form a cluster\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 2: Epsilon Circle (ε-Neighbor Concept)\n",
        "\n",
        "- Har point ke around **ε distance ka imaginary circle** draw karte hain\n",
        "- Jitne points us circle ke andar aate hain, unhe check karte hain:\n",
        "\n",
        "(\n",
        "Imagine:\n",
        "\n",
        "  (P) → Current point\n",
        "\n",
        "  Radius = ε = 1 unit\n",
        "\n",
        "  Points inside this ε-circle → Neighboring points)\n",
        "\n",
        "\n",
        " # 🔍 Step-by-Step: DBSCAN Clustering (Density-Based Spatial Clustering of Applications with Noise)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 3: Classify Points\n",
        "\n",
        "DBSCAN har point ko 3 categories me divide karta hai:\n",
        "\n",
        "| 🧠 Point Type    | ✅ Condition                                     | 📌 Meaning               |\n",
        "|------------------|--------------------------------------------------|--------------------------|\n",
        "| ✅ Core Point     | ε-radius me **MinPts ya zyada points** ho        | Cluster ka center point |\n",
        "| 🟡 Border Point   | ε-radius me **MinPts se kam**, but cluster ka hissa ho | Edge point           |\n",
        "| ❌ Noise Point    | Na core, na border → **outlier**                | Cluster ke bahar ka point |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Step 4: Cluster Expansion\n",
        "\n",
        "- Clustering **core point se start hota hai**\n",
        "- Uske neighbors ko **recursively explore** kiya jata hai\n",
        "- Jab tak naye core points milte hain → cluster **expand** hota rehta hai\n",
        "\n",
        "📌 Cluster tab tak grow karta hai jab tak naye dense regions milte rahein\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Importance of Epsilon (ε)\n",
        "\n",
        "| Condition               | Result                                 |\n",
        "|-------------------------|----------------------------------------|\n",
        "| ε **bahut chhota**       | Zyada points **noise ban** jaate hain   |\n",
        "| ε **bahut bada**         | Clusters **merge ho** jaate hain       |\n",
        "\n",
        "✅ Isiliye ε ko carefully tune karna hota hai  \n",
        "📉 Usually **k-distance graph** se best ε decide kiya jata hai (similar to elbow method)\n",
        "\n",
        "---\n",
        "\n",
        "### 🌍 Where to Use DBSCAN?\n",
        "\n",
        "| 🌐 Domain         | 📌 Use Case                                     |\n",
        "|-------------------|-------------------------------------------------|\n",
        "| 📍 Geolocation     | Cluster locations (e.g., delivery points)       |\n",
        "| 🌌 Astronomy       | Find star clusters or galaxies                  |\n",
        "| 🏦 Banking         | Detect fraud/outlier transactions              |\n",
        "| 🎓 Education       | Unusual student behavior detect karna          |\n",
        "| 🧠 Psychology      | Cluster similar thought/behavior patterns       |\n",
        "\n",
        "---\n",
        "\n",
        "### 🕐 When to Use DBSCAN?\n",
        "\n",
        "Use DBSCAN when:\n",
        "- ✅ Cluster ki shape **irregular / non-spherical** ho\n",
        "- ✅ **Outliers detect** karne ho\n",
        "- ✅ Clusters ka count (K) **define nahi karna ho**\n",
        "\n",
        "---\n",
        "### ✅ Real-Life Examples of DBSCAN\n",
        "\n",
        "| 🗂️ Area              | 🔍 Example Use Case                              |\n",
        "|----------------------|--------------------------------------------------|\n",
        "| 🗺️ Maps              | Group GPS locations (delivery clusters)         |\n",
        "| 🧪 Bioinformatics    | Group similar gene/DNA sequences                 |\n",
        "| 🏦 Credit/Fraud      | Detect credit card fraud transactions           |\n",
        "| 🌌 Astronomy         | Cluster stars or galaxies                        |\n",
        "| 🚗 Traffic Analytics | Detect accident-prone zones                      |\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Important Notes\n",
        "\n",
        "- ✅ **Recommended**: `MinPts ≥ dimensions + 1`\n",
        "- 📉 Use **k-distance graph** to choose best ε (epsilon)\n",
        "- ⚠️ DBSCAN **struggles** when clusters have **very different densities**\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Summary Table – DBSCAN Power 💪\n",
        "\n",
        "| ⚙️ Feature             | ✅ DBSCAN Advantage                          |\n",
        "|------------------------|----------------------------------------------|\n",
        "| ❌ No K Required        | Doesn’t need predefined number of clusters   |\n",
        "| ✅ Arbitrary Shapes     | Works on circular, moon, spiral clusters     |\n",
        "| ✅ Outlier Detection    | Automatically identifies noise               |\n",
        "| ✅ Non-Parametric       | No shape/distribution assumption needed      |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1MCQ4HRq3_Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prymRaad5EAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}