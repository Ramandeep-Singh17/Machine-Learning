# ğŸ“˜ Supervised Machine Learning â€” Regression (Hinglish GitHub Notes)

---

## ğŸ§  Supervised Machine Learning kya hota hai?

Supervised Machine Learning ek aisa technique hai jisme model ko **labeled data** diya jata hai â€” yaani **input (X)** aur **output (Y)** dono known hote hain.

### ğŸ“Œ Kaise kaam karta hai:

* **Input:** X (features)
* **Output:** Y (target/label)
* Model `X` aur `Y` ke relation ko learn karta hai
* Fir naye `X` pe `Y` predict karta hai

### ğŸ§ª Supervised Learning ke Types:

| Type           | Description                         | Example                         |
| -------------- | ----------------------------------- | ------------------------------- |
| Regression     | Jab output **continuous** ho        | Salary prediction, house price  |
| Classification | Jab output **categorical/label** ho | Spam detection, tumor detection |

### âœ… Kyu Important Hai?

* Output already known hota hai â†’ training easy hoti hai
* Real-world problems mostly supervised hote hain
* ML ka foundation hai

### ğŸŒ Applications:

| Domain     | Use Case                           |
| ---------- | ---------------------------------- |
| Finance    | Stock prediction, fraud detection  |
| Healthcare | Disease detection (classification) |
| E-commerce | Product recommendation             |
| Email      | Spam vs Non-Spam filtering         |
| Marketing  | Customer churn prediction          |

---

## ğŸ“ˆ Linear Regression

Linear Regression ek supervised learning algorithm hai jo **continuous output** predict karta hai.

### ğŸ“Œ Best-Fit Line:

> Goal: ek aisi line `y = mx + c` nikalna jo data ko best represent kare

* `y` = predicted output
* `x` = input
* `m` = slope (1 unit x badhne pe y kitna badhega)
* `c` = intercept (jab x = 0 ho to y kitna hai)

#### ğŸ–Šï¸ Example:

> Experience ke basis pe salary predict karna

![Best Fit Line](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

---

## ğŸ“‰ Residuals & Cost Function

### ğŸ“Œ Residual:

> Actual value aur predicted value ka difference:

```
Residual = Actual y - Predicted y
```

### ğŸ“Œ Cost Function:

> Har prediction error ka total loss calculate karta hai (generally MSE)

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}_i - y_i)^2
$$

ğŸ¯ Goal: `J(Î¸)` ko minimize karna by adjusting slope & intercept.

---

## ğŸ” Gradient Descent

> Cost function ko minimize karne ke liye use hota hai

### âš™ï¸ Kaise kaam karta hai:

* Random values se `Î¸â‚€`, `Î¸â‚` start karte hain
* Cost ka gradient calculate karte hain
* Gradually `Î¸` update karte hain minimum error ke direction me

![Gradient Descent](https://upload.wikimedia.org/wikipedia/commons/e/ec/Gradient_descent.svg)

### âš ï¸ Learning Rate (Î±):

* Ye batata hai ki step kitna bada hoga
* Zyada bada â†’ overshoot
* Zyada chhota â†’ slow learning

---

## ğŸ§  Global Minima

Cost curve ka wo point jaha error sabse kam hota hai â€” Gradient Descent wahi tak pahuchta hai.

---

## ğŸ¤– Multiple Linear Regression

Jab ek se zyada input features ho, tab use hota hai:

```
y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

Example: Age, BMI, smoker status se insurance charges predict karna

---

## ğŸ§ª Project: Insurance Charges Prediction

**Goal**: Linear Regression se insurance cost predict karna

### Steps:

1. `pd.read_csv()` se data load karo
2. `.head()`, `.info()` se data explore karo
3. `sns.scatterplot()` se visualize karo
4. Categorical features encode karo (e.g. smoker â†’ 0/1)
5. `train_test_split()` se split karo
6. `LinearRegression()` se model train karo
7. `.score()` se performance evaluate karo

ğŸ“Š Example Code:

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
X = df[['age', 'bmi', 'smoker']]
y = df['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)
```

---

## ğŸ“Š Evaluation Metrics

| Metric      | Explanation (Hindi)                            |
| ----------- | ---------------------------------------------- |
| RÂ² Score    | Model kitna variance explain karta hai         |
| Adjusted RÂ² | Feature ke count ke hisaab se adjust karta hai |
| MAE         | Mean Absolute Error                            |
| MSE         | Mean Squared Error                             |
| RMSE        | Root Mean Squared Error                        |

### ğŸ“ˆ RÂ² vs Adjusted RÂ²:

| RÂ²   | Adjusted RÂ² | Features Count | Meaning            |
| ---- | ----------- | -------------- | ------------------ |
| 0.81 | 0.79        | 1              | Good fit           |
| 0.88 | 0.86        | 2              | Better             |
| 0.99 | 0.96        | 3              | May be overfitting |

---

## ğŸš§ Underfitting vs Overfitting

| Condition    | Training Accuracy | Test Accuracy | Bias | Variance |
| ------------ | ----------------- | ------------- | ---- | -------- |
| Underfitting | âŒ Low             | âŒ Low         | High | Low      |
| Overfitting  | âœ… High            | âŒ Low         | Low  | High     |
| Good Fit     | âœ… High            | âœ… High        | Low  | Low      |

![Overfitting vs Underfitting](https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitting.svg)

---

## ğŸ§± Regularization (Overfitting se bachne ka tareeka)

### ğŸ“Œ Types:

* **Ridge (L2)**: Î¸Â² ka penalty add karta hai
* **Lasso (L1)**: |Î¸| ka penalty add karta hai

```
Cost = MSE + Î» * (Î£|Î¸|)   â† Lasso
Cost = MSE + Î» * (Î£Î¸Â²)    â† Ridge
```

---

