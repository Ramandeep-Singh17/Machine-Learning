# ğŸ“˜ Supervised Machine Learning â€” Regression (Complete Markdown Notes for GitHub)

---

## ğŸ§  What is Supervised Machine Learning?

Supervised Machine Learning is an approach where the model is trained on **labeled data** â€” that is, both **inputs (X)** and their corresponding **outputs (Y)** are known.

### ğŸ“Œ How it Works:

* **Input:** Feature(s) (X)
* **Output:** Label (Y)
* The model learns the mapping `f(X) â†’ Y`
* Then it predicts Y for new/unseen X

### ğŸ§ª Types of Supervised Learning:

| Type           | Description                      | Example                              |
| -------------- | -------------------------------- | ------------------------------------ |
| Regression     | Output is **continuous**         | Salary prediction, house price       |
| Classification | Output is **categorical/labels** | Spam detection, tumor classification |

### âœ… Why is it Important?

* Output is **clearly known** â†’ more effective training
* Covers most real-world problems (finance, healthcare, etc.)
* Foundation for most ML algorithms

### ğŸŒ Applications:

| Domain     | Use Case                           |
| ---------- | ---------------------------------- |
| Finance    | Stock prediction, fraud detection  |
| Healthcare | Disease detection (classification) |
| E-commerce | Product recommendation             |
| Email      | Spam vs Non-Spam filtering         |
| Marketing  | Customer churn prediction          |

---

## ğŸ“ˆ Linear Regression

Linear Regression is a supervised ML algorithm used to predict a **continuous target variable** using one or more input variables.

### ğŸ“Œ Best-Fit Line:

> The goal is to find the **best-fit line**: `y = mx + c`

Where:

* `y` = predicted value
* `x` = input feature
* `m` = slope (how much y changes per unit x)
* `c` = intercept (value of y when x = 0)

#### ğŸ–Šï¸ Example:

> Predicting salary (`y`) based on years of experience (`x`).

![Best Fit Line](https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg)

---

## ğŸ“‰ Residuals & Cost Function

### ğŸ“Œ Residual:

> The **difference** between the actual value and the predicted value:

```
Residual = Actual y - Predicted y
```

### ğŸ“Œ Cost Function:

> Measures total error over all training points (commonly used: **MSE**)

$$
J(\theta) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}_i - y_i)^2
$$

Goal: **Minimize cost function** by adjusting slope & intercept.

---

## ğŸ” Gradient Descent

> Optimization algorithm used to **minimize the cost function**

### âš™ï¸ How it works:

* Start with random `Î¸â‚€` and `Î¸â‚`
* Calculate gradient (slope of cost function)
* Update `Î¸` values to move toward **global minimum**

### ğŸ“‰ Visual:

![Gradient Descent](https://upload.wikimedia.org/wikipedia/commons/e/ec/Gradient_descent.svg)

### âš ï¸ Learning Rate (Î±):

* Controls step size in each iteration
* Too small â†’ slow learning
* Too large â†’ may overshoot minimum

---

## ğŸ§  Global Minima

* The point on the cost curve where the error is **minimum**.
* Gradient Descent aims to reach this point by updating parameters iteratively.

---

## ğŸ¤– Multiple Linear Regression

> When we have **multiple input features**, it's called **Multiple Regression**.

Equation:

```
y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
```

Use case: Predicting insurance cost based on age, BMI, smoker status, etc.

---

## ğŸ§ª Project: Predicting Insurance Charges

**Goal**: Predict medical insurance charges using linear regression.

### Steps:

1. Load dataset using `pd.read_csv()`
2. Explore data with `.info()`, `.head()`, `.describe()`
3. Visualize relationships (e.g., `sns.scatterplot()` for BMI vs Charges)
4. Encode categorical variables (e.g., smoker: yes/no â†’ 1/0)
5. Split data using `train_test_split()`
6. Train model using `LinearRegression()` from `sklearn`
7. Predict and evaluate with `.score()` (RÂ²)

ğŸ“Š Example Code:

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
X = df[['age', 'bmi', 'smoker']]
y = df['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)
```

---

## ğŸ“Š Model Evaluation Metrics

| Metric      | Use                                  |
| ----------- | ------------------------------------ |
| RÂ² Score    | % of variance explained by the model |
| Adjusted RÂ² | Adjusted for number of features      |
| MAE         | Mean of absolute errors              |
| MSE         | Mean of squared errors               |
| RMSE        | Square root of MSE                   |

### ğŸ“ˆ RÂ² vs Adjusted RÂ²:

| RÂ²   | Adjusted RÂ² | Features Count | Interpretation |
| ---- | ----------- | -------------- | -------------- |
| 0.81 | 0.79        | 1              | Good fit       |
| 0.88 | 0.86        | 2              | Better         |
| 0.99 | 0.96        | 3              | Overfit risk   |

---

## ğŸš§ Underfitting vs Overfitting

| Condition    | Training Accuracy | Test Accuracy | Bias | Variance |
| ------------ | ----------------- | ------------- | ---- | -------- |
| Underfitting | âŒ Low             | âŒ Low         | High | Low      |
| Overfitting  | âœ… High            | âŒ Low         | Low  | High     |
| Ideal Fit    | âœ… High            | âœ… High        | Low  | Low      |

![Overfitting vs Underfitting](https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitting.svg)

---

## ğŸ§± Regularization

> Techniques to avoid overfitting by **penalizing large coefficients**

### ğŸ“Œ Types:

* **Ridge Regression (L2)**: Adds squared penalty to cost function
* **Lasso Regression (L1)**: Adds absolute value penalty

Equation:

```
Cost = MSE + Î» * (Î£|Î¸|)  â† Lasso
Cost = MSE + Î» * (Î£Î¸Â²)   â† Ridge
```

---



*âœ… Combined from handwritten notes, Collab comments, and PDF project. Ready for GitHub.*
